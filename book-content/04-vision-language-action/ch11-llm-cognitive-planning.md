# Chapter 11: LLM Cognitive Planning

**Date**: December 16, 2025
**Module**: Vision-Language-Action (VLA) - Cognitive Intelligence
**Chapter**: 11 of 12
**Estimated Reading Time**: 150 minutes
**Prerequisites**: Module 1-4 knowledge, understanding of LLM capabilities, planning algorithm knowledge, safety system concepts

## Learning Objectives

By the end of this chapter, you will be able to:

1. Integrate Large Language Models with robot planning systems for cognitive decision-making
2. Implement cognitive planning architectures that decompose complex tasks into executable actions
3. Design task decomposition strategies that leverage LLM reasoning capabilities
4. Optimize LLM-based planning for real-time performance with robotic systems
5. Implement safety validation layers for LLM-driven robot actions
6. Create human-aware planning that considers social and ethical implications

---

## 11.1 Introduction to LLM-Based Cognitive Planning in Physical AI

### The Role of LLMs in Physical AI Planning

Large Language Models (LLMs) represent a paradigm shift in cognitive planning for Physical AI systems. Unlike traditional planning approaches that rely on predefined symbolic representations and rigid logical rules, LLMs provide the ability to reason about complex, real-world scenarios with common-sense understanding, analogical reasoning, and the ability to handle ambiguous or incomplete information.

For Physical AI systems, LLMs serve as high-level cognitive planners that can interpret natural language commands, decompose complex tasks into executable sequences, and adapt to novel situations using their vast knowledge of the physical world. This capability is crucial because physical robots must operate in environments where symbolic representations are often insufficient, and physical commonsense reasoning is required.

The integration of LLMs with Physical AI planning addresses several key challenges:

**Complex Task Decomposition**: LLMs can break down high-level, natural language commands like "Prepare tea for two people" into specific robot actions, considering appropriate subtasks and their dependencies.

**Physical Commonsense**: LLMs have learned significant knowledge about physical properties, affordances, and causal relationships through their training, enabling robots to reason about physical interactions.

**Adaptability**: LLMs can adapt to novel situations by drawing analogies to known scenarios, making them valuable for robots that must operate in diverse, changing environments.

**Human-Aware Planning**: LLMs can consider human social norms, preferences, and safety when planning robot behavior.

### Cognitive Planning vs. Traditional Planning

Traditional robot planning typically follows symbolic approaches:

- **STRIPS Planning**: Uses symbolic states and actions with preconditions and effects
- **Task and Motion Planning (TAMP)**: Integrates high-level task planning with low-level motion planning
- **Hierarchical Task Networks (HTN)**: Uses predefined hierarchical decompositions

These approaches are effective for well-structured environments with known objects and actions, but they struggle with:

- Ambiguous or natural language goals
- Novel situations not covered in symbolic models
- Integration of physical commonsense knowledge
- Human-aware planning considerations

LLM cognitive planning complements these traditional approaches by providing:

- **Natural Language Interface**: Direct interpretation of human commands
- **Commonsense Reasoning**: Understanding of physical and social common sense
- **Adaptive Task Decomposition**: Breaking down tasks based on context and capabilities
- **World Knowledge Integration**: Leveraging learned knowledge about objects, affordances, and procedures

### Architecture for LLM Cognitive Planning

The architecture for LLM cognitive planning typically follows a hybrid approach that combines LLM reasoning with traditional planning components:

```
Natural Language Input → LLM Task Decomposition → Traditional Planner/Executor → Robot Actions
                                    ↓
                            Context/State Updates ← Robot Feedback
```

With safety validation and human oversight layers throughout the process.

### Challenges in LLM-Based Planning

LLM cognitive planning faces several challenges that must be addressed in Physical AI systems:

**Reliability**: LLMs can generate incorrect or unsafe plans that could harm the robot or environment

**Real-time Constraints**: LLM inference can be slow, potentially creating unacceptable delays

**Grounding**: Abstract LLM plans must be grounded in real-world robot capabilities and states

**Verification**: Plans generated by LLMs need validation before execution

**Consistency**: LLMs may generate different plans for similar situations, affecting system reliability

---

## 11.2 LLM Integration with Planning Systems

### Planning Interface Design

The interface between LLMs and robotic planning systems must handle translation between high-level cognitive reasoning and low-level executable actions. This requires careful design of prompts, context representation, and plan validation.

```python
import openai
import json
import asyncio
from typing import Dict, List, Any, Optional
import time
from dataclasses import dataclass

@dataclass
class RobotAction:
    """Data structure for robot actions"""
    name: str
    parameters: Dict[str, Any]
    description: str
    preconditions: List[str]
    effects: List[str]

@dataclass
class PlanStep:
    """Data structure for planning steps"""
    step_id: str
    action: RobotAction
    context_info: Dict[str, Any]
    expected_outcomes: List[str]
    confidence: float

class LLMPlanningInterface:
    def __init__(self, api_key: str, model_name: str = "gpt-4"):
        self.client = openai.OpenAI(api_key=api_key)
        self.model_name = model_name
        
        # Robot capabilities and environment information
        self.robot_capabilities = self._load_robot_capabilities()
        self.environment_context = self._load_environment_context()
        
        # Planning history for context
        self.plan_history = []
        
    def _load_robot_capabilities(self) -> Dict[str, Any]:
        """Load robot capabilities and available actions"""
        return {
            "locomotion": {
                "actions": ["navigate_to", "move_to_position", "follow_path"],
                "constraints": {"max_speed": 1.0, "max_distance": 10.0, "accuracy": 0.1}
            },
            "manipulation": {
                "actions": ["grasp_object", "release_object", "place_object", "pick_up"],
                "constraints": {"max_weight": 5.0, "reach_range": 1.5, "precision": 0.01}
            },
            "perception": {
                "actions": ["detect_objects", "localize_object", "identify_person", "measure_distance"],
                "constraints": {"detection_range": 5.0, "accuracy": 0.05}
            }
        }
    
    def _load_environment_context(self) -> Dict[str, Any]:
        """Load environment-specific information"""
        return {
            "locations": {
                "kitchen": {"x": 5.0, "y": 3.0, "description": "Food preparation and storage area"},
                "living_room": {"x": 2.0, "y": 1.0, "description": "Seating and entertainment area"},
                "bedroom": {"x": 8.0, "y": 1.0, "description": "Sleeping and relaxation area"},
                "office": {"x": 4.0, "y": 6.0, "description": "Work and computer area"}
            },
            "objects": {
                "table": {"type": "furniture", "surface": True, "capacity": 5},
                "chair": {"type": "furniture", "seating": True, "capacity": 1},
                "cup": {"type": "container", "capacity": 0.25, "material": "ceramic"},
                "plate": {"type": "container", "capacity": 0.1, "material": "ceramic"},
                "bottle": {"type": "container", "capacity": 1.0, "material": "plastic"}
            }
        }
    
    def decompose_task(self, task_description: str, current_state: Dict[str, Any]) -> List[PlanStep]:
        """
        Use LLM to decompose a high-level task into executable steps
        """
        # Construct the prompt with context
        prompt = self._construct_decomposition_prompt(
            task_description, 
            current_state,
            self.robot_capabilities,
            self.environment_context
        )
        
        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": self._get_system_prompt()},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,  # Lower temperature for more consistent outputs
                max_tokens=2000,
                response_format={"type": "json_object"}
            )
            
            # Parse the response
            plan_data = json.loads(response.choices[0].message.content)
            plan_steps = self._parse_plan_steps(plan_data, current_state)
            
            # Add to history
            self.plan_history.append({
                "task": task_description,
                "plan": plan_steps,
                "timestamp": time.time()
            })
            
            return plan_steps
            
        except Exception as e:
            print(f"Error in LLM task decomposition: {e}")
            return []
    
    def _construct_decomposition_prompt(self, task_desc: str, current_state: Dict[str, Any], 
                                      capabilities: Dict[str, Any], context: Dict[str, Any]) -> str:
        """Construct prompt for task decomposition"""
        return f"""
        Task: {task_desc}
        
        Current Robot State: {json.dumps(current_state, indent=2)}
        
        Robot Capabilities: {json.dumps(capabilities, indent=2)}
        
        Environment Context: {json.dumps(context, indent=2)}
        
        Please decompose this task into a sequence of executable robot actions. Return your response as a JSON object with the following structure:
        
        {{
            "task_decomposition": [
                {{
                    "step_id": "unique_identifier",
                    "action_name": "action_to_perform",
                    "parameters": {{"param1": "value1", "param2": "value2"}},
                    "description": "Brief description of what this step accomplishes",
                    "expected_outcome": "What should happen after this step",
                    "confidence": 0.0 to 1.0
                }}
            ],
            "reasoning": "Brief explanation of how you decomposed the task"
        }}
        
        The actions must be from the robot's available capabilities and should consider:
        1. Physical constraints and affordances
        2. Safety considerations
        3. Logical sequence of operations
        4. Environmental context
        """
    
    def _get_system_prompt(self) -> str:
        """System prompt for guiding LLM behavior"""
        return """
        You are an expert cognitive planner for a physical robot. Your role is to decompose high-level tasks into executable robot actions. Follow these guidelines:

        1. Always consider physical constraints and safety
        2. Use only the provided robot capabilities and environment context
        3. Ensure logical sequence of operations (e.g., navigate before manipulation)
        4. Be specific about object and location references
        5. Consider common sense physical relationships
        6. Provide confidence scores based on feasibility given the context
        7. Think step by step about the task decomposition
        """
    
    def _parse_plan_steps(self, plan_data: Dict[str, Any], current_state: Dict[str, Any]) -> List[PlanStep]:
        """Parse the LLM response into PlanStep objects"""
        steps = []
        
        for step_data in plan_data.get("task_decomposition", []):
            # Validate action is available
            action_name = step_data["action_name"]
            valid_action = self._validate_action(action_name)
            
            if not valid_action:
                print(f"Warning: Invalid action '{action_name}' in plan, skipping")
                continue
            
            action = RobotAction(
                name=action_name,
                parameters=step_data.get("parameters", {}),
                description=step_data.get("description", ""),
                preconditions=[],  # Would be populated based on action type
                effects=[]  # Would be populated based on action type
            )
            
            plan_step = PlanStep(
                step_id=step_data.get("step_id", f"step_{len(steps)}"),
                action=action,
                context_info={
                    "current_state": current_state,
                    "reasoning": plan_data.get("reasoning", "")
                },
                expected_outcomes=[step_data.get("expected_outcome", "")],
                confidence=step_data.get("confidence", 0.5)
            )
            
            steps.append(plan_step)
        
        return steps
    
    def _validate_action(self, action_name: str) -> bool:
        """Validate if action is supported by the robot"""
        for category, details in self.robot_capabilities.items():
            if action_name in details["actions"]:
                return True
        return False

# Example usage
# llm_planner = LLMPlanningInterface(api_key="your-openai-api-key")
# current_robot_state = {
#     "location": {"x": 2.0, "y": 1.0},
#     "held_object": None,
#     "battery_level": 0.8
# }
# plan = llm_planner.decompose_task("Go to the kitchen and get a glass of water", current_robot_state)
```

### Context Integration and World Modeling

LLM cognitive planning requires effective integration of real-time sensor data and environmental context to ground abstract reasoning in the physical world.

```python
class ContextIntegrator:
    def __init__(self, llm_planner: LLMPlanningInterface):
        self.llm_planner = llm_planner
        self.spatial_memory = {}
        self.object_memory = {}
        self.temporal_memory = {}
        self.uncertainty_tracker = {}
    
    def build_context(self, robot_state: Dict[str, Any], perception_data: Dict[str, Any]) -> Dict[str, Any]:
        """Build comprehensive context for LLM planning"""
        context = {
            "robot_state": self._format_robot_state(robot_state),
            "perceived_environment": self._format_perception_data(perception_data),
            "spatial_knowledge": self._get_spatial_knowledge(),
            "object_knowledge": self._get_object_knowledge(),
            "temporal_context": self._get_temporal_context(),
            "uncertainty_assessment": self._assess_uncertainty(perception_data),
            "previous_actions": self._get_recent_actions(),
            "user_intent": self._get_user_intent()  # Would come from voice interface
        }
        
        return context
    
    def _format_robot_state(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """Format robot state for context"""
        return {
            "location": state.get("location", {"x": 0, "y": 0, "theta": 0}),
            "battery": state.get("battery_level", 1.0),
            "held_object": state.get("held_object"),
            "active_tasks": state.get("active_tasks", []),
            "capabilities_status": state.get("capabilities_status", {}),
            "current_plan": state.get("current_plan", None)
        }
    
    def _format_perception_data(self, perception: Dict[str, Any]) -> Dict[str, Any]:
        """Format perception data for context"""
        return {
            "detected_objects": [
                {
                    "name": obj.get("name", "unknown"),
                    "type": obj.get("type", "object"),
                    "location": obj.get("location", {}),
                    "confidence": obj.get("confidence", 0.0),
                    "properties": obj.get("properties", {})
                }
                for obj in perception.get("objects", [])
            ],
            "spatial_map": perception.get("spatial_map", {}),
            "human_detection": perception.get("humans", []),
            "obstacle_map": perception.get("obstacles", [])
        }
    
    def _get_spatial_knowledge(self) -> Dict[str, Any]:
        """Retrieve spatial knowledge and relationships"""
        return {
            "known_locations": self.llm_planner.environment_context["locations"],
            "spatial_relations": self.spatial_memory,
            "navigation_history": []
        }
    
    def _get_object_knowledge(self) -> Dict[str, Any]:
        """Retrieve object knowledge and affordances"""
        return {
            "known_objects": self.llm_planner.environment_context["objects"],
            "object_affordances": self.object_memory,
            "object_locations": {}
        }
    
    def _get_temporal_context(self) -> Dict[str, Any]:
        """Retrieve temporal context and schedule"""
        return {
            "current_time": time.time(),
            "recent_events": self.temporal_memory.get("events", [])[-10:],  # Last 10 events
            "temporal_constraints": {}
        }
    
    def _assess_uncertainty(self, perception_data: Dict[str, Any]) -> Dict[str, Any]:
        """Assess uncertainty in perception data"""
        uncertainty = {
            "object_detection_confidence": self._calculate_detection_confidence(perception_data),
            "spatial_uncertainty": self._calculate_spatial_uncertainty(perception_data),
            "dynamic_object_risk": self._assess_dynamic_risk(perception_data)
        }
        
        return uncertainty
    
    def _calculate_detection_confidence(self, perception_data: Dict[str, Any]) -> float:
        """Calculate overall detection confidence"""
        objects = perception_data.get("objects", [])
        if not objects:
            return 0.1  # Low confidence if no objects detected
        
        avg_confidence = sum(obj.get("confidence", 0.0) for obj in objects) / len(objects)
        return avg_confidence
    
    def _calculate_spatial_uncertainty(self, perception_data: Dict[str, Any]) -> float:
        """Calculate spatial mapping uncertainty"""
        # This would integrate with SLAM uncertainty quantification
        return 0.05  # Placeholder value
    
    def _assess_dynamic_risk(self, perception_data: Dict[str, Any]) -> float:
        """Assess risk from dynamic objects (people, moving obstacles)"""
        humans = perception_data.get("humans", [])
        return min(1.0, len(humans) * 0.2)  # Simple risk calculation
    
    def _get_recent_actions(self) -> List[Dict[str, Any]]:
        """Get recent actions for temporal context"""
        if self.llm_planner.plan_history:
            # Return recent plan steps from history
            recent_plan = self.llm_planner.plan_history[-1] if self.llm_planner.plan_history else {}
            return recent_plan.get("plan", [])[-5:]  # Last 5 steps
        return []
    
    def _get_user_intent(self) -> str:
        """Get recent user intent (would come from voice interface)"""
        # This would connect to voice processing system
        return "default_intent"
    
    def update_context(self, sensor_data: Dict[str, Any]):
        """Update context with new sensor data"""
        # Update spatial memory
        self._update_spatial_memory(sensor_data)
        
        # Update object memory
        self._update_object_memory(sensor_data)
        
        # Update temporal memory
        self._update_temporal_memory(sensor_data)
        
        # Update uncertainty assessments
        self._update_uncertainty_assessment(sensor_data)
    
    def _update_spatial_memory(self, sensor_data: Dict[str, Any]):
        """Update spatial knowledge with new data"""
        # Implementation would update spatial relationships and locations
        pass
    
    def _update_object_memory(self, sensor_data: Dict[str, Any]):
        """Update object knowledge with new data"""
        # Implementation would update object locations and properties
        pass
    
    def _update_temporal_memory(self, sensor_data: Dict[str, Any]):
        """Update temporal context with new data"""
        event = {
            "timestamp": time.time(),
            "data": sensor_data,
            "event_type": "sensor_update"
        }
        if "events" not in self.temporal_memory:
            self.temporal_memory["events"] = []
        self.temporal_memory["events"].append(event)
    
    def _update_uncertainty_assessment(self, sensor_data: Dict[str, Any]):
        """Update uncertainty assessments"""
        # Implementation would update uncertainty models
        pass
```

### Plan Validation and Safety Checking

LLM-generated plans must be validated for safety and feasibility before execution in physical environments.

```python
class PlanValidator:
    def __init__(self):
        self.safety_rules = self._load_safety_rules()
        self.feasibility_checks = self._load_feasibility_checks()
        self.ethics_filters = self._load_ethics_filters()
    
    def _load_safety_rules(self) -> Dict[str, Any]:
        """Load safety rules and constraints"""
        return {
            "collision_avoidance": {
                "function": self._check_collision_safety,
                "priority": 10
            },
            "human_safety": {
                "function": self._check_human_safety,
                "priority": 10
            },
            "robot_limits": {
                "function": self._check_robot_limits,
                "priority": 8
            },
            "environmental_safety": {
                "function": self._check_environmental_safety,
                "priority": 8
            }
        }
    
    def _load_feasibility_checks(self) -> Dict[str, Any]:
        """Load feasibility validation functions"""
        return {
            "action_feasibility": {
                "function": self._check_action_feasibility,
                "priority": 9
            },
            "resource_availability": {
                "function": self._check_resource_availability,
                "priority": 7
            },
            "precondition_satisfaction": {
                "function": self._check_preconditions,
                "priority": 9
            }
        }
    
    def _load_ethics_filters(self) -> Dict[str, Any]:
        """Load ethical consideration filters"""
        return {
            "privacy_protection": {
                "function": self._check_privacy_compliance,
                "priority": 6
            },
            "respect_for_autonomy": {
                "function": self._check_respect_for_human_autonomy,
                "priority": 7
            },
            "fairness_consideration": {
                "function": self._check_fairness,
                "priority": 5
            }
        }
    
    def validate_plan(self, plan: List[PlanStep], context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Validate an LLM-generated plan for safety and feasibility
        """
        validation_results = {
            "overall_validity": True,
            "safety_issues": [],
            "feasibility_issues": [],
            "ethical_issues": [],
            "warnings": [],
            "suggested_modifications": []
        }
        
        # Check each safety rule
        for rule_name, rule_config in self.safety_rules.items():
            result = rule_config["function"](plan, context)
            if not result["valid"]:
                validation_results["overall_validity"] = False
                validation_results["safety_issues"].append({
                    "rule": rule_name,
                    "issue": result["issue"],
                    "severity": result["severity"],
                    "suggestion": result.get("suggestion", "")
                })
        
        # Check each feasibility rule
        for check_name, check_config in self.feasibility_checks.items():
            result = check_config["function"](plan, context)
            if not result["valid"]:
                validation_results["overall_validity"] = False
                validation_results["feasibility_issues"].append({
                    "check": check_name,
                    "issue": result["issue"],
                    "severity": result["severity"],
                    "suggestion": result.get("suggestion", "")
                })
        
        # Check ethical considerations
        for filter_name, filter_config in self.ethics_filters.items():
            result = filter_config["function"](plan, context)
            if not result["valid"]:
                validation_results["warnings"].append({
                    "filter": filter_name,
                    "issue": result["issue"],
                    "severity": result["severity"],
                    "suggestion": result.get("suggestion", "")
                })
        
        return validation_results
    
    def _check_collision_safety(self, plan: List[PlanStep], context: Dict[str, Any]) -> Dict[str, Any]:
        """Check for collision safety in navigation plans"""
        robot_location = context["robot_state"]["location"]
        obstacles = context["perceived_environment"]["obstacle_map"]
        
        for step in plan:
            if step.action.name in ["navigate_to", "move_to_position"]:
                target_location = step.action.parameters.get("target_position")
                if target_location:
                    # Simple collision check - in reality, this would use path planning
                    path_collision = self._check_path_for_collision(
                        robot_location, target_location, obstacles
                    )
                    
                    if path_collision:
                        return {
                            "valid": False,
                            "issue": f"Collision detected on path to {target_location}",
                            "severity": "critical",
                            "suggestion": "Recalculate path avoiding obstacles"
                        }
        
        return {"valid": True, "issue": None, "severity": "none"}
    
    def _check_human_safety(self, plan: List[PlanStep], context: Dict[str, Any]) -> Dict[str, Any]:
        """Check for human safety in plans"""
        humans = context["perceived_environment"]["human_detection"]
        
        for step in plan:
            if step.action.name in ["navigate_to", "move_to_position"]:
                target_location = step.action.parameters.get("target_position")
                
                # Check if target is too close to humans
                for human in humans:
                    human_pos = human.get("location", {})
                    if self._calculate_distance(target_location, human_pos) < 0.5:  # 50cm safety distance
                        return {
                            "valid": False,
                            "issue": f"Action too close to human at {human_pos}",
                            "severity": "critical",
                            "suggestion": "Maintain safe distance from humans"
                        }
        
        return {"valid": True, "issue": None, "severity": "none"}
    
    def _check_robot_limits(self, plan: List[PlanStep], context: Dict[str, Any]) -> Dict[str, Any]:
        """Check if actions are within robot capabilities"""
        capabilities = context.get("robot_capabilities", {})
        
        for step in plan:
            action_name = step.action.name
            params = step.action.parameters
            
            # Check manipulation limits
            if action_name in ["grasp_object", "pick_up"]:
                if "weight" in params:
                    max_weight = capabilities.get("manipulation", {}).get("constraints", {}).get("max_weight", 5.0)
                    if params["weight"] > max_weight:
                        return {
                            "valid": False,
                            "issue": f"Object weight {params['weight']} exceeds limit {max_weight}",
                            "severity": "critical",
                            "suggestion": "Find alternative object or get assistance"
                        }
        
        return {"valid": True, "issue": None, "severity": "none"}
    
    def _check_action_feasibility(self, plan: List[PlanStep], context: Dict[str, Any]) -> Dict[str, Any]:
        """Check if actions are feasible given current state"""
        current_state = context["robot_state"]
        
        for step in plan:
            action_name = step.action.name
            
            # Check if robot is capable of action
            if action_name == "grasp_object" and current_state["held_object"]:
                return {
                    "valid": False,
                    "issue": "Cannot grasp object while already holding one",
                    "severity": "high",
                    "suggestion": "Release current object first or use different action"
                }
            
            # Check navigation feasibility
            if action_name == "navigate_to":
                target = step.action.parameters.get("target")
                if not self._location_is_navigable(target):
                    return {
                        "valid": False,
                        "issue": f"Target location {target} is not navigable",
                        "severity": "high",
                        "suggestion": "Choose accessible location or request assistance"
                    }
        
        return {"valid": True, "issue": None, "severity": "none"}
    
    def _check_preconditions(self, plan: List[PlanStep], context: Dict[str, Any]) -> Dict[str, Any]:
        """Check if preconditions for actions are satisfied"""
        # This would implement more sophisticated precondition checking
        # For now, basic checks
        current_state = context["robot_state"]
        
        for step in plan:
            action_name = step.action.name
            params = step.action.parameters
            
            if action_name == "place_object" and not current_state["held_object"]:
                return {
                    "valid": False,
                    "issue": "Cannot place object without holding one",
                    "severity": "high",
                    "suggestion": "Grasp an object first"
                }
        
        return {"valid": True, "issue": None, "severity": "none"}
    
    def _check_path_for_collision(self, start: Dict[str, float], end: Dict[str, float], obstacles: List[Dict]) -> bool:
        """Simple collision check for path (in reality, use proper path planning)"""
        # This is a simplified check - real implementation would use proper path planning
        for obstacle in obstacles:
            if self._is_on_path(start, end, obstacle.get("location", {})):
                return True
        return False
    
    def _is_on_path(self, start: Dict[str, float], end: Dict[str, float], point: Dict[str, float]) -> bool:
        """Check if point is on path between start and end (simplified)"""
        # Simplified implementation - real version would use proper path analysis
        path_vector = [end["x"] - start["x"], end["y"] - start["y"]]
        point_vector = [point["x"] - start["x"], point["y"] - start["y"]]
        
        # Calculate distance from point to path (simplified)
        path_length = (path_vector[0]**2 + path_vector[1]**2)**0.5
        if path_length > 0:
            projection = (point_vector[0]*path_vector[0] + point_vector[1]*path_vector[1]) / path_length**2
            if 0 <= projection <= 1:  # Point is between start and end
                closest_x = start["x"] + projection * path_vector[0]
                closest_y = start["y"] + projection * path_vector[1]
                distance = ((point["x"] - closest_x)**2 + (point["y"] - closest_y)**2)**0.5
                return distance < 0.3  # 30cm threshold
        
        return False
    
    def _calculate_distance(self, pos1: Dict[str, float], pos2: Dict[str, float]) -> float:
        """Calculate Euclidean distance between two positions"""
        return ((pos1["x"] - pos2["x"])**2 + (pos1["y"] - pos2["y"])**2)**0.5
    
    def _location_is_navigable(self, location: str) -> bool:
        """Check if location is navigable (simplified)"""
        # In reality, this would check against navigation map
        return True  # Simplified - assume all locations are navigable
```

---

## 11.3 Task Decomposition and Hierarchical Planning

### Hierarchical Task Networks with LLMs

LLM cognitive planning can implement hierarchical task decomposition, breaking complex goals into manageable subtasks while maintaining coherence and addressing dependencies.

```python
from typing import NamedTuple
import networkx as nx  # For dependency graphs

class TaskNode(NamedTuple):
    """Node representing a task in the hierarchy"""
    task_id: str
    description: str
    action: Optional[RobotAction]
    dependencies: List[str]
    subtasks: List['TaskNode']
    confidence: float
    priority: int
    execution_time_estimate: float

class HierarchicalTaskDecomposer:
    def __init__(self, llm_planner: LLMPlanningInterface):
        self.llm_planner = llm_planner
        self.task_graph = nx.DiGraph()
        
    def decompose_task_hierarchically(self, high_level_task: str, context: Dict[str, Any]) -> TaskNode:
        """
        Decompose a high-level task hierarchically using LLM reasoning
        """
        # First, get initial decomposition from LLM
        initial_plan = self.llm_planner.decompose_task(high_level_task, context["robot_state"])
        
        # Convert to hierarchical structure
        if not initial_plan:
            # If LLM didn't work, create simple fallback
            return TaskNode(
                task_id="fallback",
                description=high_level_task,
                action=None,
                dependencies=[],
                subtasks=[],
                confidence=0.5,
                priority=5,
                execution_time_estimate=10.0
            )
        
        # Build hierarchical structure
        root_task = self._build_hierarchical_structure(high_level_task, initial_plan, context)
        
        return root_task
    
    def _build_hierarchical_structure(self, root_description: str, plan_steps: List[PlanStep], 
                                    context: Dict[str, Any]) -> TaskNode:
        """Build hierarchical task structure from plan steps"""
        # Group related steps into subtasks
        subtasks = self._group_related_steps(plan_steps)
        
        # Create dependency graph
        dependencies = self._analyze_dependencies(plan_steps)
        
        # Build the root task
        root_task = TaskNode(
            task_id=f"root_{hash(root_description)}",
            description=root_description,
            action=None,  # Root tasks don't have direct actions
            dependencies=[],
            subtasks=subtasks,
            confidence=self._calculate_hierarchical_confidence(plan_steps),
            priority=10,  # Highest level task
            execution_time_estimate=sum(step.action.parameters.get("estimated_time", 5.0) for step in plan_steps)
        )
        
        return root_task
    
    def _group_related_steps(self, plan_steps: List[PlanStep]) -> List[TaskNode]:
        """Group related plan steps into logical subtasks"""
        subtasks = []
        
        # Simple grouping strategy: group by action category
        action_groups = {}
        for step in plan_steps:
            category = self._categorize_action(step.action.name)
            if category not in action_groups:
                action_groups[category] = []
            action_groups[category].append(step)
        
        for category, steps in action_groups.items():
            if len(steps) == 1:
                # Single step becomes a leaf task
                step = steps[0]
                subtask = TaskNode(
                    task_id=f"step_{len(subtasks)}_{step.action.name}",
                    description=step.action.description,
                    action=step.action,
                    dependencies=[],
                    subtasks=[],
                    confidence=step.confidence,
                    priority=5,
                    execution_time_estimate=5.0  # Default estimate
                )
            else:
                # Multiple steps become a compound subtask
                subtask = TaskNode(
                    task_id=f"compound_{len(subtasks)}_{category}",
                    description=f"{category} related actions",
                    action=None,
                    dependencies=[],
                    subtasks=[TaskNode(
                        task_id=f"step_{i}_{step.action.name}",
                        description=step.action.description,
                        action=step.action,
                        dependencies=[],
                        subtasks=[],
                        confidence=step.confidence,
                        priority=5,
                        execution_time_estimate=5.0
                    ) for i, step in enumerate(steps)],
                    confidence=min(step.confidence for step in steps),
                    priority=5,
                    execution_time_estimate=sum(5.0 for _ in steps)
                )
            
            subtasks.append(subtask)
        
        return subtasks
    
    def _categorize_action(self, action_name: str) -> str:
        """Categorize action for grouping purposes"""
        navigation_actions = ['navigate_to', 'move_to_position', 'follow_path', 'go_to']
        manipulation_actions = ['grasp_object', 'pick_up', 'place_object', 'release_object']
        
        if action_name in navigation_actions:
            return "navigation"
        elif action_name in manipulation_actions:
            return "manipulation"
        else:
            return "other"
    
    def _analyze_dependencies(self, plan_steps: List[PlanStep]) -> Dict[str, List[str]]:
        """Analyze dependencies between steps"""
        dependencies = {}
        
        # Simple dependency analysis: some actions require others to complete first
        for i, step in enumerate(plan_steps):
            deps = []
            
            # Example dependencies
            if step.action.name in ['grasp_object', 'pick_up']:
                # Need to navigate to object first
                for j, prev_step in enumerate(plan_steps[:i]):
                    if (prev_step.action.name in ['navigate_to', 'localize_object'] and
                        'object' in str(prev_step.action.parameters)):
                        deps.append(prev_step.step_id)
            
            elif step.action.name == 'place_object':
                # Need to pick up object first
                for j, prev_step in enumerate(plan_steps[:i]):
                    if prev_step.action.name in ['grasp_object', 'pick_up']:
                        deps.append(prev_step.step_id)
            
            dependencies[step.step_id] = deps
        
        return dependencies
    
    def _calculate_hierarchical_confidence(self, plan_steps: List[PlanStep]) -> float:
        """Calculate confidence for hierarchical task"""
        if not plan_steps:
            return 0.5
        
        avg_confidence = sum(step.confidence for step in plan_steps) / len(plan_steps)
        
        # Adjust based on plan complexity
        complexity_factor = min(1.0, len(plan_steps) / 10.0)  # More steps = lower confidence
        adjusted_confidence = avg_confidence * (1.0 - 0.1 * complexity_factor)
        
        return max(0.1, adjusted_confidence)
    
    def execute_hierarchical_task(self, task_node: TaskNode, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Execute hierarchical task with proper sequencing and dependency management
        """
        execution_results = {
            "success": True,
            "task_id": task_node.task_id,
            "results": [],
            "execution_time": 0.0,
            "failed_tasks": []
        }
        
        start_time = time.time()
        
        # Execute subtasks in dependency order
        ordered_subtasks = self._order_tasks_by_dependencies(task_node.subtasks, 
                                                           self._get_dependencies_for_subtasks(task_node))
        
        for subtask in ordered_subtasks:
            if subtask.action:  # Leaf task with direct action
                result = self._execute_single_action(subtask.action, context)
                execution_results["results"].append(result)
                
                if not result.get("success", False):
                    execution_results["success"] = False
                    execution_results["failed_tasks"].append(subtask.task_id)
                    # Depending on configuration, might continue or fail fast
                    # For now, continue to see full execution results
            else:  # Compound subtask
                result = self.execute_hierarchical_task(subtask, context)
                execution_results["results"].append(result)
                
                if not result.get("success", False):
                    execution_results["success"] = False
                    execution_results["failed_tasks"].extend(result.get("failed_tasks", []))
        
        execution_results["execution_time"] = time.time() - start_time
        return execution_results
    
    def _order_tasks_by_dependencies(self, subtasks: List[TaskNode], dependencies: Dict[str, List[str]]) -> List[TaskNode]:
        """Order tasks based on dependencies using topological sort"""
        # Create dependency graph
        graph = nx.DiGraph()
        
        # Add nodes
        for task in subtasks:
            graph.add_node(task.task_id, task_node=task)
        
        # Add edges based on dependencies
        for task_id, deps in dependencies.items():
            for dep in deps:
                graph.add_edge(dep, task_id)
        
        # Topological sort
        ordered_ids = list(nx.topological_sort(graph))
        ordered_tasks = []
        
        for task_id in ordered_ids:
            task_node = graph.nodes[task_id]['task_node']
            ordered_tasks.append(task_node)
        
        return ordered_tasks
    
    def _get_dependencies_for_subtasks(self, task_node: TaskNode) -> Dict[str, List[str]]:
        """Extract dependencies for subtasks (simplified implementation)"""
        # In reality, this would use a more sophisticated dependency tracking system
        return {subtask.task_id: subtask.dependencies for subtask in task_node.subtasks}
    
    def _execute_single_action(self, action: RobotAction, context: Dict[str, Any]) -> Dict[str, Any]:
        """Execute a single robot action (simplified simulation)"""
        print(f"Executing action: {action.name} with parameters: {action.parameters}")
        
        # Simulate action execution
        # In reality, this would interface with robot execution system
        import random
        success = random.random() > 0.1  # 90% success rate for simulation
        
        # Simulate execution time
        execution_time = action.parameters.get("estimated_time", 5.0)
        
        return {
            "action": action.name,
            "parameters": action.parameters,
            "success": success,
            "execution_time": execution_time,
            "result": "simulated_result" if success else "execution_failed"
        }
```

### Adaptive Task Refinement

LLM cognitive planning systems must be able to refine and adapt plans based on execution feedback and changing conditions.

```python
class AdaptiveTaskRefiner:
    def __init__(self, llm_planner: LLMPlanningInterface):
        self.llm_planner = llm_planner
        self.execution_history = []
        self.failure_patterns = {}
        self.adaptation_strategies = self._load_adaptation_strategies()
    
    def _load_adaptation_strategies(self) -> Dict[str, Any]:
        """Load strategies for adapting plans based on failures"""
        return {
            "retry_with_backoff": {
                "function": self._apply_retry_backoff,
                "applicable_to": ["navigation", "manipulation"]
            },
            "alternative_approach": {
                "function": self._apply_alternative_approach,
                "applicable_to": ["navigation", "perception"]
            },
            "resource_substitution": {
                "function": self._apply_resource_substitution,
                "applicable_to": ["manipulation", "navigation"]
            },
            "task_simplification": {
                "function": self._apply_task_simplification,
                "applicable_to": ["complex_tasks"]
            }
        }
    
    def adapt_plan(self, original_plan: List[PlanStep], execution_result: Dict[str, Any], 
                   context: Dict[str, Any]) -> List[PlanStep]:
        """
        Adapt plan based on execution results and context
        """
        # Analyze execution results
        failure_analysis = self._analyze_execution_failures(execution_result)
        
        if not failure_analysis["has_failures"]:
            return original_plan  # No adaptation needed
        
        # Determine appropriate adaptation strategy
        strategy = self._select_adaptation_strategy(failure_analysis, original_plan)
        
        if strategy:
            adapted_plan = strategy["function"](original_plan, failure_analysis, context)
            return adapted_plan
        
        # If no specific strategy applies, try general adaptation
        return self._general_plan_adaptation(original_plan, failure_analysis, context)
    
    def _analyze_execution_failures(self, execution_result: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze execution results to identify failures and their causes"""
        analysis = {
            "has_failures": False,
            "failed_steps": [],
            "failure_types": [],
            "failure_causes": {},
            "success_rate": 0.0
        }
        
        if not execution_result.get("results"):
            return analysis
        
        total_steps = len(execution_result["results"])
        successful_steps = 0
        
        for i, result in enumerate(execution_result["results"]):
            step_success = result.get("success", False)
            
            if not step_success:
                analysis["has_failures"] = True
                analysis["failed_steps"].append(i)
                
                # Categorize failure type
                failure_type = self._categorize_failure(result)
                analysis["failure_types"].append(failure_type)
                
                # Analyze cause
                cause = self._analyze_failure_cause(result)
                analysis["failure_causes"][i] = cause
            else:
                successful_steps += 1
        
        analysis["success_rate"] = successful_steps / total_steps if total_steps > 0 else 0.0
        
        return analysis
    
    def _categorize_failure(self, result: Dict[str, Any]) -> str:
        """Categorize type of failure"""
        error_msg = result.get("error", "").lower()
        
        if "collision" in error_msg:
            return "collision_failure"
        elif "timeout" in error_msg:
            return "timeout_failure" 
        elif "cannot" in error_msg or "unable" in error_msg:
            return "capability_failure"
        elif "not found" in error_msg or "missing" in error_msg:
            return "perception_failure"
        else:
            return "unknown_failure"
    
    def _analyze_failure_cause(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze root cause of failure"""
        action = result.get("action", "unknown")
        error = result.get("error", "")
        
        # Use LLM to help analyze failure cause
        analysis_prompt = f"""
        Action: {action}
        Error: {error}
        Context: {result.get('context', 'no context provided')}
        
        Analyze the likely cause of this robot execution failure and suggest root causes.
        Consider:
        - Environmental factors
        - Sensor/perception issues
        - Robot capability limitations
        - Planning assumptions that might be incorrect
        
        Provide analysis in JSON format:
        {{
            "root_cause": "brief description",
            "contributing_factors": ["factor1", "factor2"],
            "likelihood": "high/medium/low",
            "suggested_fix": "what to change"
        }}
        """
        
        try:
            response = self.llm_planner.client.chat.completions.create(
                model=self.llm_planner.model_name,
                messages=[
                    {"role": "system", "content": "You are an expert at analyzing robot execution failures."},
                    {"role": "user", "content": analysis_prompt}
                ],
                response_format={"type": "json_object"},
                max_tokens=500
            )
            
            analysis = json.loads(response.choices[0].message.content)
            return analysis
        except:
            # Fallback analysis
            return {
                "root_cause": "unknown",
                "contributing_factors": [],
                "likelihood": "medium",
                "suggested_fix": "retry with modified parameters"
            }
    
    def _select_adaptation_strategy(self, failure_analysis: Dict[str, Any], 
                                  original_plan: List[PlanStep]) -> Optional[Dict[str, Any]]:
        """Select appropriate adaptation strategy based on failure analysis"""
        # Choose strategy based on failure patterns
        if "collision_failure" in failure_analysis["failure_types"]:
            return self.adaptation_strategies["alternative_approach"]
        elif "timeout_failure" in failure_analysis["failure_types"]:
            return self.adaptation_strategies["retry_with_backoff"]
        elif "capability_failure" in failure_analysis["failure_types"]:
            return self.adaptation_strategies["resource_substitution"]
        elif "perception_failure" in failure_analysis["failure_types"]:
            return self.adaptation_strategies["alternative_approach"]
        
        return None
    
    def _apply_retry_backoff(self, plan: List[PlanStep], analysis: Dict[str, Any], 
                           context: Dict[str, Any]) -> List[PlanStep]:
        """Apply retry with backoff strategy"""
        adapted_plan = []
        
        for i, step in enumerate(plan):
            if i in analysis["failed_steps"]:
                # Increase tolerance or change parameters for retry
                new_params = step.action.parameters.copy()
                
                # Example: for navigation, increase tolerance
                if step.action.name == "navigate_to":
                    new_params["tolerance"] = new_params.get("tolerance", 0.1) * 2.0
                    new_params["max_retries"] = 3
                
                new_action = RobotAction(
                    name=step.action.name,
                    parameters=new_params,
                    description=step.action.description,
                    preconditions=step.action.preconditions,
                    effects=step.action.effects
                )
                
                adapted_step = PlanStep(
                    step_id=f"{step.step_id}_retry",
                    action=new_action,
                    context_info=step.context_info,
                    expected_outcomes=step.expected_outcomes,
                    confidence=step.confidence * 0.8  # Lower confidence for retry
                )
                adapted_plan.append(adapted_step)
            else:
                adapted_plan.append(step)
        
        return adapted_plan
    
    def _apply_alternative_approach(self, plan: List[PlanStep], analysis: Dict[str, Any], 
                                  context: Dict[str, Any]) -> List[PlanStep]:
        """Apply alternative approach strategy"""
        adapted_plan = []
        
        for i, step in enumerate(plan):
            if i in analysis["failed_steps"]:
                failure_cause = analysis["failure_causes"].get(i, {})
                
                # Try to find alternative action based on failure cause
                alternative_action = self._find_alternative_action(step, failure_cause, context)
                
                if alternative_action:
                    adapted_step = PlanStep(
                        step_id=f"{step.step_id}_alt",
                        action=alternative_action,
                        context_info=step.context_info,
                        expected_outcomes=step.expected_outcomes,
                        confidence=step.confidence * 0.7  # Lower confidence for alternative
                    )
                    adapted_plan.append(adapted_step)
                else:
                    # Keep original step if no good alternative found
                    adapted_plan.append(step)
            else:
                adapted_plan.append(step)
        
        return adapted_plan
    
    def _find_alternative_action(self, original_step: PlanStep, failure_cause: Dict[str, Any], 
                               context: Dict[str, Any]) -> Optional[RobotAction]:
        """Find alternative action for failed step"""
        original_action = original_step.action
        
        # Define alternative mappings
        alternatives = {
            "navigate_to": ["go_around", "use_alternative_path", "request_guidance"],
            "grasp_object": ["use_different_gripper", "change_approach_angle", "request_assistance"],
            "perceive_object": ["change_viewpoint", "use_different_sensor", "move_closer"]
        }
        
        for alt_name in alternatives.get(original_action.name, []):
            if self._is_action_feasible(alt_name, context):
                # Create alternative action with modified parameters
                alt_params = original_action.parameters.copy()
                
                if alt_name == "go_around":
                    alt_params["strategy"] = "obstacle_avoidance"
                elif alt_name == "use_alternative_path":
                    alt_params["path_type"] = "safe"
                
                return RobotAction(
                    name=alt_name,
                    parameters=alt_params,
                    description=f"Alternative to {original_action.name}",
                    preconditions=original_action.preconditions,
                    effects=original_action.effects
                )
        
        return None
    
    def _is_action_feasible(self, action_name: str, context: Dict[str, Any]) -> bool:
        """Check if action is feasible in current context"""
        capabilities = context.get("robot_capabilities", {})
        
        for category, details in capabilities.items():
            if action_name in details.get("actions", []):
                return True
        
        return False
    
    def _general_plan_adaptation(self, plan: List[PlanStep], analysis: Dict[str, Any], 
                               context: Dict[str, Any]) -> List[PlanStep]:
        """Apply general plan adaptation when specific strategies don't apply"""
        # Use LLM to suggest plan modifications based on failure analysis
        adaptation_prompt = f"""
        Original Plan: {json.dumps([{
            'step_id': step.step_id,
            'action': step.action.name,
            'params': step.action.parameters,
            'confidence': step.confidence
        } for step in plan], indent=2)}

        Failure Analysis: {json.dumps(analysis, indent=2)}

        Current Context: {json.dumps(context, indent=2)}

        Suggest modifications to this plan to address the failures while maintaining the overall goal.
        Return the modified plan in the same format as the original.
        """
        
        try:
            response = self.llm_planner.client.chat.completions.create(
                model=self.llm_planner.model_name,
                messages=[
                    {"role": "system", "content": "You are an expert at adapting robot plans to address execution failures."},
                    {"role": "user", "content": adaptation_prompt}
                ],
                response_format={"type": "json_object"},
                max_tokens=1000
            )
            
            suggestion = json.loads(response.choices[0].message.content)
            
            # Parse the modified plan
            modified_steps = []
            for step_data in suggestion.get("modified_plan", []):
                action = RobotAction(
                    name=step_data["action"],
                    parameters=step_data.get("params", {}),
                    description=step_data.get("description", ""),
                    preconditions=[],
                    effects=[]
                )
                
                adapted_step = PlanStep(
                    step_id=step_data.get("step_id", f"adapted_{len(modified_steps)}"),
                    action=action,
                    context_info=context,
                    expected_outcomes=[],
                    confidence=step_data.get("confidence", 0.5)
                )
                
                modified_steps.append(adapted_step)
            
            return modified_steps
            
        except Exception as e:
            print(f"Error in general plan adaptation: {e}")
            return plan  # Return original plan if adaptation fails
```

---

## 11.4 Real-Time Planning and Performance Optimization

### LLM Caching and Optimization

Real-time LLM cognitive planning requires optimization strategies to reduce latency and improve response times for critical robot operations.

```python
import hashlib
from typing import Tuple
import pickle
import os
from datetime import datetime, timedelta

class LLMOptimizer:
    def __init__(self, cache_size: int = 1000, cache_dir: str = "/tmp/llm_cache"):
        self.cache_size = cache_size
        self.cache_dir = cache_dir
        self.cache_file = os.path.join(cache_dir, "planning_cache.pkl")
        
        # Create cache directory
        os.makedirs(cache_dir, exist_ok=True)
        
        # Load existing cache
        self.cache = self._load_cache()
        self.access_times = {}  # Track access times for LRU
        
        # Performance metrics
        self.query_count = 0
        self.cache_hits = 0
        self.cache_misses = 0
        
    def _load_cache(self) -> Dict[str, Tuple[Any, datetime]]:
        """Load cache from file"""
        if os.path.exists(self.cache_file):
            try:
                with open(self.cache_file, 'rb') as f:
                    return pickle.load(f)
            except:
                return {}
        return {}
    
    def _save_cache(self):
        """Save cache to file"""
        try:
            with open(self.cache_file, 'wb') as f:
                pickle.dump(self.cache, f)
        except Exception as e:
            print(f"Error saving cache: {e}")
    
    def get_cached_result(self, prompt: str) -> Optional[Any]:
        """Get cached result for prompt if available"""
        self.query_count += 1
        
        # Create cache key from prompt
        cache_key = self._create_cache_key(prompt)
        
        if cache_key in self.cache:
            result, timestamp = self.cache[cache_key]
            
            # Check if cache is still valid (not too old)
            if datetime.now() - timestamp < timedelta(hours=24):  # Cache valid for 24 hours
                self.cache_hits += 1
                self.access_times[cache_key] = datetime.now()
                return result
        
        self.cache_misses += 1
        return None
    
    def cache_result(self, prompt: str, result: Any):
        """Cache result for prompt"""
        cache_key = self._create_cache_key(prompt)
        
        # Update access time
        self.access_times[cache_key] = datetime.now()
        
        # Add to cache
        self.cache[cache_key] = (result, datetime.now())
        
        # Implement LRU if cache is too large
        if len(self.cache) > self.cache_size:
            self._apply_lru_eviction()
        
        # Save cache periodically
        if len(self.cache) % 10 == 0:  # Save every 10 additions
            self._save_cache()
    
    def _create_cache_key(self, prompt: str) -> str:
        """Create a cache key from prompt"""
        return hashlib.md5(prompt.encode()).hexdigest()
    
    def _apply_lru_eviction(self):
        """Apply LRU eviction to maintain cache size"""
        if len(self.cache) <= self.cache_size:
            return
        
        # Sort by access time (oldest first)
        sorted_items = sorted(self.access_times.items(), key=lambda x: x[1])
        
        # Remove oldest entries
        remove_count = len(self.cache) - self.cache_size + 10  # Remove slightly more than needed
        
        for i in range(min(remove_count, len(sorted_items))):
            key_to_remove = sorted_items[i][0]
            if key_to_remove in self.cache:
                del self.cache[key_to_remove]
                del self.access_times[key_to_remove]
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """Get performance statistics"""
        if self.query_count == 0:
            hit_rate = 0
        else:
            hit_rate = self.cache_hits / self.query_count
        
        return {
            "cache_size": len(self.cache),
            "query_count": self.query_count,
            "cache_hits": self.cache_hits,
            "cache_misses": self.cache_misses,
            "hit_rate": hit_rate,
            "avg_cache_age": self._get_avg_cache_age()
        }
    
    def _get_avg_cache_age(self) -> timedelta:
        """Get average age of cache entries"""
        if not self.cache:
            return timedelta(0)
        
        total_age = timedelta(0)
        for _, timestamp in self.cache.values():
            total_age += datetime.now() - timestamp
        
        return total_age / len(self.cache) if self.cache else timedelta(0)

class OptimizedLLMPlanningInterface(LLMPlanningInterface):
    def __init__(self, api_key: str, model_name: str = "gpt-4", enable_cache: bool = True):
        super().__init__(api_key, model_name)
        
        self.enable_cache = enable_cache
        if enable_cache:
            self.optimizer = LLMOptimizer(cache_size=500)
    
    def decompose_task(self, task_description: str, current_state: Dict[str, Any]) -> List[PlanStep]:
        """
        Optimized task decomposition with caching
        """
        if self.enable_cache:
            # Create cache key from task and context
            cache_context = {
                "task": task_description,
                "capabilities": self.robot_capabilities,
                "environment": self.environment_context
            }
            cache_key = f"{task_description}_{hash(str(sorted(current_state.items())))}"
            
            # Try to get from cache first
            cached_result = self.optimizer.get_cached_result(cache_key)
            if cached_result is not None:
                print(f"Cache hit for task: {task_description[:50]}...")
                return cached_result
        
        # Fall back to LLM call
        result = super().decompose_task(task_description, current_state)
        
        # Cache the result if caching is enabled
        if self.enable_cache:
            self.optimizer.cache_result(cache_key, result)
        
        return result
```

### Parallel Planning and Execution

For real-time operation, LLM cognitive planning can benefit from parallel processing and speculative execution.

```python
import concurrent.futures
import threading
from queue import Queue

class ParallelLLMPlanner:
    def __init__(self, num_workers: int = 4, timeout: float = 30.0):
        self.num_workers = num_workers
        self.timeout = timeout
        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=num_workers)
        self.planning_queue = Queue()
        self.result_cache = {}
        self.active_plans = {}
        
    def plan_multiple_tasks(self, tasks: List[Tuple[str, Dict[str, Any]]]) -> Dict[str, List[PlanStep]]:
        """
        Plan multiple tasks in parallel
        """
        # Submit all tasks to executor
        futures = []
        task_ids = []
        
        for i, (task_desc, context) in enumerate(tasks):
            task_id = f"task_{i}_{hash(task_desc)}"
            future = self.executor.submit(self._plan_single_task, task_desc, context, task_id)
            futures.append(future)
            task_ids.append(task_id)
        
        # Collect results
        results = {}
        for i, (future, task_id) in enumerate(zip(futures, task_ids)):
            try:
                result = future.result(timeout=self.timeout)
                results[task_id] = result
            except concurrent.futures.TimeoutError:
                print(f"Task {task_ids[i]} timed out")
                results[task_id] = []
            except Exception as e:
                print(f"Error planning task {task_ids[i]}: {e}")
                results[task_id] = []
        
        return results
    
    def _plan_single_task(self, task_desc: str, context: Dict[str, Any], task_id: str) -> List[PlanStep]:
        """
        Plan a single task (to be run in parallel)
        """
        # This would use the regular planning interface
        # For this example, we'll simulate
        import random
        import time
        
        # Simulate planning time
        time.sleep(random.uniform(0.1, 0.5))
        
        # Return a simple plan structure
        return [PlanStep(
            step_id=f"{task_id}_step_1",
            action=RobotAction(
                name="mock_action",
                parameters={"task": task_desc},
                description=f"Action for {task_desc}",
                preconditions=[],
                effects=[]
            ),
            context_info={},
            expected_outcomes=[f"Complete {task_desc}"],
            confidence=0.8
        )]
    
    def speculative_planning(self, current_task: str, context: Dict[str, Any], 
                           likely_followup_tasks: List[str]) -> Dict[str, List[PlanStep]]:
        """
        Plan for likely follow-up tasks while executing current task
        """
        all_tasks = [current_task] + likely_followup_tasks
        task_contexts = [context] * len(all_tasks)  # Use same context for all tasks
        
        # Plan all tasks in parallel
        results = self.plan_multiple_tasks(list(zip(all_tasks, task_contexts)))
        
        # Separate current task result from speculative plans
        current_result = results.get(f"task_0_{hash(current_task)}", [])
        speculative_results = {
            task: results.get(f"task_{i+1}_{hash(task)}", []) 
            for i, task in enumerate(likely_followup_tasks)
        }
        
        return {
            "current_plan": current_result,
            "speculative_plans": speculative_results
        }

class RealTimeCognitivePlanner:
    def __init__(self, llm_planner: OptimizedLLMPlanningInterface):
        self.llm_planner = llm_planner
        self.parallel_planner = ParallelLLMPlanner()
        self.context_integrator = ContextIntegrator(llm_planner)
        self.plan_validator = PlanValidator()
        
        # Real-time performance metrics
        self.planning_times = []
        self.execution_times = []
        self.cache_hit_rates = []
        
    def plan_with_real_time_constraints(self, task_description: str, 
                                      current_state: Dict[str, Any],
                                      max_planning_time: float = 2.0) -> Dict[str, Any]:
        """
        Plan with real-time constraints and fallback mechanisms
        """
        start_time = time.time()
        
        # Get context
        perception_data = current_state.get("perception_data", {})
        context = self.context_integrator.build_context(current_state, perception_data)
        
        # Try full planning first
        try:
            plan = self.llm_planner.decompose_task(task_description, current_state)
            
            # Validate plan
            validation = self.plan_validator.validate_plan(plan, context)
            
            if validation["overall_validity"]:
                planning_time = time.time() - start_time
                return {
                    "success": True,
                    "plan": plan,
                    "validation": validation,
                    "planning_time": planning_time,
                    "source": "llm_planning"
                }
            else:
                # Plan validation failed, try fallback
                return self._execute_fallback_planning(task_description, current_state, validation)
                
        except Exception as e:
            print(f"LLM planning failed: {e}, using fallback")
            return self._execute_fallback_planning(task_description, current_state)
    
    def _execute_fallback_planning(self, task_description: str, current_state: Dict[str, Any],
                                 validation_result: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        Execute fallback planning when main planning fails
        """
        # Simple fallback: basic navigation or manipulation if mentioned
        if "go to" in task_description.lower() or "navigate" in task_description.lower():
            # Extract location if possible
            location = self._extract_location_from_task(task_description)
            if location:
                simple_plan = [PlanStep(
                    step_id="fallback_navigate",
                    action=RobotAction(
                        name="navigate_to",
                        parameters={"target": location},
                        description=f"Navigate to {location}",
                        preconditions=[],
                        effects=[]
                    ),
                    context_info={"original_task": task_description},
                    expected_outcomes=[f"Reach {location}"],
                    confidence=0.6
                )]
                
                return {
                    "success": True,
                    "plan": simple_plan,
                    "validation": {"overall_validity": True, "fallback_used": True},
                    "planning_time": 0.1,  # Very fast fallback
                    "source": "fallback"
                }
        
        # No suitable fallback found
        return {
            "success": False,
            "error": "No suitable plan could be generated",
            "task": task_description
        }
    
    def _extract_location_from_task(self, task_description: str) -> Optional[str]:
        """Extract location from task description"""
        # Simple keyword matching (in reality, use more sophisticated NLP)
        locations = ["kitchen", "living room", "bedroom", "office", "dining room"]
        
        desc_lower = task_description.lower()
        for loc in locations:
            if loc in desc_lower:
                return loc
        
        return None
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """Get real-time performance metrics"""
        return {
            "avg_planning_time": sum(self.planning_times) / len(self.planning_times) if self.planning_times else 0,
            "avg_execution_time": sum(self.execution_times) / len(self.execution_times) if self.execution_times else 0,
            "cache_hit_rate": self.llm_planner.optimizer.get_performance_stats()["hit_rate"] if hasattr(self.llm_planner, 'optimizer') else 0,
            "recent_plans_count": len(self.planning_times),
            "validation_success_rate": 0  # Would track validation results
        }
```

---

## 11.5 Safety and Validation Layers

### Comprehensive Safety Framework

Implementing a multi-layered safety framework is crucial for LLM cognitive planning in Physical AI systems.

```python
class SafetyFramework:
    def __init__(self):
        self.safety_layers = [
            self._physical_safety_checker,
            self._ethical_decision_checker,
            self._human_safety_checker,
            self._environmental_safety_checker
        ]
    
    def _physical_safety_checker(self, plan: List[PlanStep], context: Dict[str, Any]) -> Dict[str, Any]:
        """Check for physical safety violations"""
        issues = []
        
        for step in plan:
            if step.action.name == "navigate_to":
                target = step.action.parameters.get("target_position")
                if target:
                    # Check if target is in safe area (not too high, dangerous location, etc.)
                    if self._is_unsafe_location(target):
                        issues.append({
                            "type": "physical_safety",
                            "issue": f"Target location {target} is not physically safe",
                            "severity": "critical",
                            "action": "stop_and_report"
                        })
        
        return {
            "valid": len(issues) == 0,
            "issues": issues,
            "layer": "physical_safety"
        }
    
    def _ethical_decision_checker(self, plan: List[PlanStep], context: Dict[str, Any]) -> Dict[str, Any]:
        """Check for ethical violations"""
        issues = []
        
        for step in plan:
            # Check for privacy violations (recording in private areas)
            if step.action.name in ["perceive_object", "detect_objects"] and self._in_private_area(step, context):
                issues.append({
                    "type": "privacy_ethics",
                    "issue": "Action involves perception in private area without consent",
                    "severity": "high",
                    "action": "request_permission"
                })
        
        return {
            "valid": len(issues) == 0,
            "issues": issues,
            "layer": "ethical_decision"
        }
    
    def _human_safety_checker(self, plan: List[PlanStep], context: Dict[str, Any]) -> Dict[str, Any]:
        """Check for human safety issues"""
        issues = []
        
        humans = context.get("perceived_environment", {}).get("human_detection", [])
        
        for step in plan:
            if step.action.name in ["navigate_to", "move_to_position"]:
                target = step.action.parameters.get("target_position")
                if target:
                    for human in humans:
                        human_pos = human.get("location", {})
                        distance = self._calculate_distance(target, human_pos)
                        if distance < 0.5:  # 50cm safety distance
                            issues.append({
                                "type": "human_safety",
                                "issue": f"Action target too close to human at {human_pos}",
                                "severity": "critical",
                                "action": "maintain_safe_distance"
                            })
        
        return {
            "valid": len(issues) == 0,
            "issues": issues,
            "layer": "human_safety"
        }
    
    def _environmental_safety_checker(self, plan: List[PlanStep], context: Dict[str, Any]) -> Dict[str, Any]:
        """Check for environmental safety issues"""
        issues = []
        
        for step in plan:
            if step.action.name == "manipulate_object":
                obj_name = step.action.parameters.get("object_name", "")
                obj_props = step.action.parameters.get("object_properties", {})
                
                # Check if object is fragile or dangerous
                if obj_props.get("fragile", False):
                    issues.append({
                        "type": "environmental_safety",
                        "issue": f"Object {obj_name} is fragile and may break",
                        "severity": "medium",
                        "action": "use_careful_manipulation"
                    })
        
        return {
            "valid": len(issues) == 0,
            "issues": issues,
            "layer": "environmental_safety"
        }
    
    def _is_unsafe_location(self, location: Dict[str, float]) -> bool:
        """Check if location is physically unsafe (simplified)"""
        # Check if location is too high, in dangerous area, etc.
        return location.get("z", 0) > 2.0  # Too high
    
    def _in_private_area(self, step: PlanStep, context: Dict[str, Any]) -> bool:
        """Check if action is in private area"""
        # This would check against privacy zone map
        return False  # Simplified
    
    def _calculate_distance(self, pos1: Dict[str, float], pos2: Dict[str, float]) -> float:
        """Calculate distance between two positions"""
        return ((pos1.get("x", 0) - pos2.get("x", 0))**2 + 
                (pos1.get("y", 0) - pos2.get("y", 0))**2)**0.5
    
    def validate_plan_safety(self, plan: List[PlanStep], context: Dict[str, Any]) -> Dict[str, Any]:
        """Validate plan safety using all safety layers"""
        safety_results = []
        all_valid = True
        
        for safety_layer in self.safety_layers:
            result = safety_layer(plan, context)
            safety_results.append(result)
            if not result["valid"]:
                all_valid = False
        
        return {
            "overall_safety": all_valid,
            "safety_results": safety_results,
            "total_issues": sum(len(result["issues"]) for result in safety_results),
            "critical_issues": sum(
                1 for result in safety_results 
                for issue in result["issues"] 
                if issue["severity"] == "critical"
            )
        }

class SafeLLMCognitivePlanner(RealTimeCognitivePlanner):
    def __init__(self, llm_planner: OptimizedLLMPlanningInterface):
        super().__init__(llm_planner)
        self.safety_framework = SafetyFramework()
    
    def plan_with_safety_validation(self, task_description: str, 
                                  current_state: Dict[str, Any]) -> Dict[str, Any]:
        """Plan task with comprehensive safety validation"""
        # Get plan from regular planner
        plan_result = self.plan_with_real_time_constraints(task_description, current_state)
        
        if not plan_result["success"]:
            return plan_result
        
        plan = plan_result["plan"]
        
        # Get context
        perception_data = current_state.get("perception_data", {})
        context = self.context_integrator.build_context(current_state, perception_data)
        
        # Validate safety
        safety_validation = self.safety_framework.validate_plan_safety(plan, context)
        
        if not safety_validation["overall_safety"]:
            # Try to adapt plan based on safety issues
            adapted_plan = self._adapt_plan_for_safety(plan, safety_validation, context)
            if adapted_plan:
                # Re-validate adapted plan
                safety_check = self.safety_framework.validate_plan_safety(adapted_plan, context)
                if safety_check["overall_safety"]:
                    plan_result["plan"] = adapted_plan
                    plan_result["safety_adapted"] = True
                else:
                    # Even adapted plan is not safe
                    plan_result["success"] = False
                    plan_result["error"] = "Plan could not be made safe"
                    plan_result["safety_issues"] = safety_validation
            else:
                # Could not adapt plan for safety
                plan_result["success"] = False
                plan_result["error"] = "Plan is unsafe and cannot be adapted"
                plan_result["safety_issues"] = safety_validation
        else:
            # Plan is safe as-is
            plan_result["safety_validated"] = True
        
        return plan_result
    
    def _adapt_plan_for_safety(self, plan: List[PlanStep], safety_validation: Dict[str, Any], 
                             context: Dict[str, Any]) -> Optional[List[PlanStep]]:
        """Adapt plan to address safety issues"""
        # This would implement specific adaptation strategies for safety
        # For now, return None to indicate adaptation not possible
        return None
```

---

## Summary

This chapter has provided a comprehensive exploration of LLM cognitive planning for Physical AI and humanoid robotics systems:

1. **LLM Integration**: Understanding the role of Large Language Models in cognitive planning and how they complement traditional planning approaches
2. **Planning Interface Design**: Creating effective interfaces between LLMs and robotic planning systems with proper context integration
3. **Task Decomposition**: Implementing hierarchical task networks and adaptive refinement strategies for complex goal achievement
4. **Performance Optimization**: Techniques for real-time operation including caching, parallel planning, and speculative execution
5. **Safety and Validation**: Comprehensive safety frameworks to ensure LLM-generated plans are safe for physical robot execution

The key insight from this chapter is that LLM cognitive planning provides powerful reasoning capabilities for Physical AI systems, but requires careful integration with traditional planning, safety validation, and real-time performance optimization to be effective in physical environments. The combination of LLM reasoning with systematic validation and adaptation creates robust cognitive planning systems.

## Key Terms

- **Cognitive Planning**: High-level planning that involves reasoning, learning, and adaptation using cognitive models
- **Large Language Model (LLM)**: Transformer-based neural networks trained on vast text corpora that can perform reasoning tasks
- **Task Decomposition**: Breaking complex goals into manageable subtasks with proper dependencies
- **Hierarchical Task Network (HTN)**: Planning approach using hierarchical task structures
- **Plan Validation**: Verification of plan correctness, safety, and feasibility before execution
- **Safety Framework**: Multi-layered system for ensuring plan safety in physical environments
- **Adaptive Planning**: Ability to modify plans based on execution feedback and changing conditions
- **Speculative Execution**: Planning for likely future tasks while executing current tasks
- **Parallel Planning**: Generating multiple plans simultaneously for efficiency
- **Context Integration**: Incorporating real-time sensor and environmental data into planning

## Further Reading

- Brown, T., et al. (2020). "Language Models are Few-Shot Learners." Advances in Neural Information Processing Systems.
- Qin, K., et al. (2022). "SayPlan: Grounding Large Language Models using 3D Scene Graphs for Robot Task Planning." arXiv preprint.
- Huang, W., et al. (2022). "Language Models as Zero-Shot Planners." International Conference on Learning Representations.
- Patel, A., et al. (2022). "MindCraft: Theory of Mind Based Planning using Large Language Models for Minecraft." arXiv preprint.
- Shah, R., et al. (2022). "Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning." arXiv preprint.

---

**Chapter 12 Preview**: In the final chapter, we will explore the capstone autonomous humanoid system that integrates all components developed throughout the book, demonstrating how Physical AI principles, modular intelligence, and real-time planning combine to create sophisticated embodied intelligence.