"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[2504],{8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>s});var o=t(6540);const i={},a=o.createContext(i);function r(e){const n=o.useContext(a);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),o.createElement(a.Provider,{value:n},e.children)}},9620:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>m,frontMatter:()=>r,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"vision-language-action/ch10-voice-to-action-integration","title":"Chapter 10: Voice-to-Action Integration","description":"Date: December 16, 2025","source":"@site/docs/04-vision-language-action/ch10-voice-to-action-integration.md","sourceDirName":"04-vision-language-action","slug":"/vision-language-action/ch10-voice-to-action-integration","permalink":"/Physical_AI_and_Humanoid_Robotics_Book/docs/vision-language-action/ch10-voice-to-action-integration","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 7-10 Source Research","permalink":"/Physical_AI_and_Humanoid_Robotics_Book/docs/digital-twin/chapter-7-10-sources"},"next":{"title":"Chapter 11: LLM Cognitive Planning","permalink":"/Physical_AI_and_Humanoid_Robotics_Book/docs/vision-language-action/ch11-llm-cognitive-planning"}}');var i=t(4848),a=t(8453);const r={},s="Chapter 10: Voice-to-Action Integration",c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"10.1 Introduction to Voice-to-Action in Physical AI",id:"101-introduction-to-voice-to-action-in-physical-ai",level:2},{value:"Voice Commands as Natural Interaction Modality",id:"voice-commands-as-natural-interaction-modality",level:3},{value:"Components of Voice-to-Action Systems",id:"components-of-voice-to-action-systems",level:3},{value:"Challenges in Physical AI Voice Integration",id:"challenges-in-physical-ai-voice-integration",level:3},{value:"Architecture for Voice-to-Action Integration",id:"architecture-for-voice-to-action-integration",level:3},{value:"10.2 Speech Recognition and Audio Processing",id:"102-speech-recognition-and-audio-processing",level:2},{value:"Modern Speech Recognition Approaches",id:"modern-speech-recognition-approaches",level:3},{value:"Audio Preprocessing for Robot Environments",id:"audio-preprocessing-for-robot-environments",level:3},{value:"Real-time Processing Considerations",id:"real-time-processing-considerations",level:3},{value:"10.3 Natural Language Understanding for Robot Commands",id:"103-natural-language-understanding-for-robot-commands",level:2},{value:"Command Structure and Intent Recognition",id:"command-structure-and-intent-recognition",level:3},{value:"Semantic Role Labeling for Action Understanding",id:"semantic-role-labeling-for-action-understanding",level:3},{value:"10.4 Voice Command Mapping to Robot Actions",id:"104-voice-command-mapping-to-robot-actions",level:2},{value:"Action Mapping Architecture",id:"action-mapping-architecture",level:3},{value:"Context-Aware Command Resolution",id:"context-aware-command-resolution",level:3},{value:"10.5 Robustness and Error Handling",id:"105-robustness-and-error-handling",level:2},{value:"Handling Recognition Errors",id:"handling-recognition-errors",level:3},{value:"Multi-Modal Error Recovery",id:"multi-modal-error-recovery",level:3},{value:"10.6 Integration with Higher-Level Planning Systems",id:"106-integration-with-higher-level-planning-systems",level:2},{value:"Voice Command Integration Architecture",id:"voice-command-integration-architecture",level:3},{value:"Summary",id:"summary",level:2},{value:"Key Terms",id:"key-terms",level:2},{value:"Further Reading",id:"further-reading",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"chapter-10-voice-to-action-integration",children:"Chapter 10: Voice-to-Action Integration"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Date"}),": December 16, 2025\n",(0,i.jsx)(n.strong,{children:"Module"}),": Vision-Language-Action (VLA) - Human-Robot Interaction\n",(0,i.jsx)(n.strong,{children:"Chapter"}),": 10 of 12\n",(0,i.jsx)(n.strong,{children:"Estimated Reading Time"}),": 150 minutes\n",(0,i.jsx)(n.strong,{children:"Prerequisites"}),": Module 1-3 knowledge, understanding of natural language processing, real-time system concepts, human-robot interaction principles"]}),"\n",(0,i.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Integrate speech recognition systems with robot control using modern approaches like Whisper"}),"\n",(0,i.jsx)(n.li,{children:"Process audio input and map voice commands to specific robot actions"}),"\n",(0,i.jsx)(n.li,{children:"Design robust voice command processing pipelines with confidence scoring"}),"\n",(0,i.jsx)(n.li,{children:"Handle real-time speech processing with low latency requirements"}),"\n",(0,i.jsx)(n.li,{children:"Implement multi-modal interaction combining voice, vision, and action"}),"\n",(0,i.jsx)(n.li,{children:"Create error handling and user feedback mechanisms for voice interfaces"}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"101-introduction-to-voice-to-action-in-physical-ai",children:"10.1 Introduction to Voice-to-Action in Physical AI"}),"\n",(0,i.jsx)(n.h3,{id:"voice-commands-as-natural-interaction-modality",children:"Voice Commands as Natural Interaction Modality"}),"\n",(0,i.jsx)(n.p,{children:"Voice-to-action integration represents a crucial capability for Physical AI systems, enabling natural human-robot interaction through spoken language. Unlike traditional command-line interfaces or application-based controls, voice commands allow humans to interact with robots using their most natural communication modality. For Physical AI systems, this means translating linguistic instructions into embodied actions in the physical world."}),"\n",(0,i.jsx)(n.p,{children:"The challenge of voice-to-action integration extends beyond simple speech recognition. It requires understanding the context of spoken commands, mapping abstract language to concrete robotic actions, and executing these actions while maintaining the natural flow of human conversation. This is particularly complex for Physical AI systems that must interpret commands in the context of their physical environment and current state."}),"\n",(0,i.jsx)(n.p,{children:'Voice-to-action systems must handle the inherent ambiguity of natural language. When a human says "Pick up the red cup," the system must determine:'}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:'What constitutes "the red cup" among potentially multiple objects'}),"\n",(0,i.jsx)(n.li,{children:'The spatial relationships needed for "picking up"'}),"\n",(0,i.jsx)(n.li,{children:"The current state of the robot and its available actions"}),"\n",(0,i.jsx)(n.li,{children:"The environmental constraints that might affect execution"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"components-of-voice-to-action-systems",children:"Components of Voice-to-Action Systems"}),"\n",(0,i.jsx)(n.p,{children:"A complete voice-to-action system typically includes several key components:"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Speech Recognition"}),": Converting audio signals to text with appropriate confidence measures"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Natural Language Understanding (NLU)"}),": Parsing the text to extract intent, entities, and action specifications"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Context Resolution"}),": Using environmental and situational context to disambiguate commands"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Action Planning"}),": Mapping high-level commands to specific robot behaviors"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Execution and Feedback"}),": Carrying out actions and providing appropriate feedback"]}),"\n",(0,i.jsx)(n.p,{children:"Each component must operate efficiently to maintain natural interaction timing, typically requiring responses within 1-2 seconds for acceptable user experience."}),"\n",(0,i.jsx)(n.h3,{id:"challenges-in-physical-ai-voice-integration",children:"Challenges in Physical AI Voice Integration"}),"\n",(0,i.jsx)(n.p,{children:"Voice integration in Physical AI systems faces several unique challenges:"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Environmental Noise"}),": Physical robot environments often have significant background noise that can interfere with speech recognition"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Distance and Directionality"}),": Microphones may be distant from speakers, and robot orientation may affect signal quality"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Real-time Requirements"}),": Physical interactions often require immediate response, limiting processing time"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Ambiguity Resolution"}),": Natural language commands may require environmental context for proper interpretation"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Safety Considerations"}),": Voice commands must be processed carefully to prevent unsafe robot behaviors"]}),"\n",(0,i.jsx)(n.h3,{id:"architecture-for-voice-to-action-integration",children:"Architecture for Voice-to-Action Integration"}),"\n",(0,i.jsx)(n.p,{children:"The architecture for voice-to-action integration typically follows a pipeline pattern with feedback mechanisms for improved performance:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Audio Input \u2192 Preprocessing \u2192 Speech Recognition \u2192 NLU \u2192 Context Resolution \u2192 Action Planning \u2192 Robot Execution\n"})}),"\n",(0,i.jsx)(n.p,{children:"With feedback loops for:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Execution results informing future command interpretation"}),"\n",(0,i.jsx)(n.li,{children:"Environmental state updates affecting command resolution"}),"\n",(0,i.jsx)(n.li,{children:"User feedback for system improvement"}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"102-speech-recognition-and-audio-processing",children:"10.2 Speech Recognition and Audio Processing"}),"\n",(0,i.jsx)(n.h3,{id:"modern-speech-recognition-approaches",children:"Modern Speech Recognition Approaches"}),"\n",(0,i.jsx)(n.p,{children:"Modern speech recognition has been revolutionized by deep learning approaches, particularly transformer-based models. OpenAI's Whisper represents one of the most effective open-source options for voice-to-text conversion, offering multilingual support and robust performance across various acoustic conditions."}),"\n",(0,i.jsx)(n.p,{children:"Whisper's architecture combines an encoder-decoder transformer with attention mechanisms, trained on large-scale multilingual and multitask learning objectives. This results in robust performance across different speakers, languages, and acoustic conditions, making it particularly suitable for robotic applications."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import whisper\nimport torch\nimport numpy as np\nimport pyaudio\nimport wave\nimport threading\nfrom queue import Queue\nimport time\n\nclass SpeechRecognizer:\n    def __init__(self, model_size="small", device="cuda" if torch.cuda.is_available() else "cpu"):\n        """\n        Initialize speech recognition system with Whisper model\n        """\n        self.model_size = model_size\n        self.device = device\n        \n        # Load Whisper model\n        self.model = whisper.load_model(model_size, device=device)\n        \n        # Audio parameters\n        self.sample_rate = 16000\n        self.chunk_size = 1024\n        self.audio_format = pyaudio.paInt16\n        self.channels = 1\n        \n        # Audio input setup\n        self.audio = pyaudio.PyAudio()\n        self.stream = None\n        \n        # Processing queue for real-time operation\n        self.audio_queue = Queue()\n        self.result_queue = Queue()\n        \n        # Recognition parameters\n        self.energy_threshold = 1000  # Minimum audio energy to consider as speech\n        self.silence_duration = 1.0   # Seconds of silence to trigger recognition\n        \n        print(f"Speech recognizer initialized with model: {model_size}, device: {device}")\n    \n    def start_listening(self):\n        """Start audio input and recognition in background thread"""\n        self.stream = self.audio.open(\n            format=self.audio_format,\n            channels=self.channels,\n            rate=self.sample_rate,\n            input=True,\n            frames_per_buffer=self.chunk_size\n        )\n        \n        # Start background thread for audio processing\n        self.listening_thread = threading.Thread(target=self._audio_input_loop, daemon=True)\n        self.listening_thread.start()\n        \n        print("Started listening for voice commands...")\n    \n    def stop_listening(self):\n        """Stop audio input and recognition"""\n        if self.stream:\n            self.stream.stop_stream()\n            self.stream.close()\n        self.audio.terminate()\n    \n    def _audio_input_loop(self):\n        """Continuous audio input and processing loop"""\n        audio_buffer = []\n        silent_chunks = 0\n        total_chunks = 0\n        silence_threshold = int(self.silence_duration * self.sample_rate / self.chunk_size)\n        \n        while True:\n            try:\n                # Read audio data\n                data = self.stream.read(self.chunk_size, exception_on_overflow=False)\n                audio_array = np.frombuffer(data, dtype=np.int16).astype(np.float32) / 32768.0\n                \n                # Calculate energy for voice detection\n                energy = np.sum(audio_array ** 2) / len(audio_array)\n                \n                if energy > self.energy_threshold:\n                    # Voice detected, add to buffer\n                    audio_buffer.extend(audio_array)\n                    silent_chunks = 0\n                    total_chunks += 1\n                else:\n                    # Silence detected\n                    if len(audio_buffer) > 0:\n                        silent_chunks += 1\n                    \n                    # If enough silence and we have collected some audio\n                    if silent_chunks >= silence_threshold and len(audio_buffer) > 0:\n                        # Process the collected audio\n                        audio_segment = np.array(audio_buffer)\n                        \n                        # Add to processing queue\n                        self.audio_queue.put(audio_segment.copy())\n                        \n                        # Clear buffer for next segment\n                        audio_buffer = []\n                        \n                        # Reset counters\n                        silent_chunks = 0\n                        total_chunks = 0\n                    elif len(audio_buffer) > 0:  # Still in speech, continue collecting\n                        audio_buffer.extend(audio_array)\n                \n                time.sleep(0.01)  # Small delay to prevent excessive CPU usage\n                \n            except Exception as e:\n                print(f"Error in audio input loop: {e}")\n                break\n    \n    def process_audio_segment(self, audio_segment):\n        """Process a single audio segment using Whisper"""\n        try:\n            # Pad audio to minimum required length for Whisper\n            if len(audio_segment) < 32000:  # Whisper minimum is ~1 second at 16kHz\n                padding = np.zeros(32000 - len(audio_segment))\n                audio_segment = np.concatenate([audio_segment, padding])\n            \n            # Run Whisper transcription\n            result = self.model.transcribe(\n                audio_segment,\n                language=\'en\',\n                temperature=0.0,  # Deterministic results\n                compression_ratio_threshold=2.4,  # Filter out low-quality transcriptions\n                logprob_threshold=-1.0,  # Filter out low-probability transcriptions\n                no_speech_threshold=0.6  # Filter out non-speech audio\n            )\n            \n            if result and result[\'text\'].strip():\n                return {\n                    \'text\': result[\'text\'].strip(),\n                    \'confidence\': self._calculate_confidence(result),\n                    \'timestamp\': time.time(),\n                    \'language\': result.get(\'language\', \'unknown\')\n                }\n        \n        except Exception as e:\n            print(f"Error processing audio segment: {e}")\n        \n        return None\n    \n    def _calculate_confidence(self, result):\n        """Calculate confidence score from Whisper results"""\n        # Use the average log probability as confidence indicator\n        if \'avg_logprob\' in result:\n            # Convert log probability to confidence (higher is better, but negative)\n            # So we use 1 - abs(avg_logprob) normalized to positive range\n            avg_logprob = result[\'avg_logprob\']\n            # Logprob is negative, so more negative = less confident\n            # Normalize to 0-1 range where 1 is most confident\n            confidence = max(0.0, min(1.0, 1.0 - abs(avg_logprob + 1.0)))\n            return confidence\n        \n        return 0.5  # Default confidence if not available\n    \n    def get_recognized_command(self, timeout=None):\n        """Get the next recognized voice command"""\n        try:\n            return self.result_queue.get(timeout=timeout)\n        except:\n            return None\n\n# Example usage\nrecognizer = SpeechRecognizer()\nrecognizer.start_listening()\n\n# In a separate thread or process, you would continuously process commands:\ndef command_processor():\n    while True:\n        if not recognizer.audio_queue.empty():\n            audio_segment = recognizer.audio_queue.get()\n            result = recognizer.process_audio_segment(audio_segment)\n            if result:\n                print(f"Recognized: {result[\'text\']} (confidence: {result[\'confidence\']:.2f})")\n                # Add to result queue for higher-level processing\n                recognizer.result_queue.put(result)\n        time.sleep(0.1)\n\n# Start command processor\nprocessor_thread = threading.Thread(target=command_processor, daemon=True)\nprocessor_thread.start()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"audio-preprocessing-for-robot-environments",children:"Audio Preprocessing for Robot Environments"}),"\n",(0,i.jsx)(n.p,{children:"Robotic environments often present challenging acoustic conditions that require specialized preprocessing. The robot's own motors, fans, and mechanical sounds can interfere with speech recognition. Additionally, ambient noise from the operating environment needs to be addressed."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import webrtcvad\nimport collections\nimport scipy.signal\nimport librosa\n\nclass AudioPreprocessor:\n    def __init__(self):\n        # Voice Activity Detection\n        self.vad = webrtcvad.Vad(2)  # Sensitivity level 2 (0-3)\n        \n        # Parameters for VAD\n        self.sample_rate = 16000\n        self.frame_duration = 30  # ms\n        self.frame_size = int(self.sample_rate * self.frame_duration / 1000)\n        \n        # Noise reduction parameters\n        self.fft_size = 2048\n        self.hop_length = 512\n        \n        # For noise estimation\n        self.noise_buffer_size = 50\n        self.noise_buffer = collections.deque(maxlen=self.noise_buffer_size)\n        \n    def preprocess_audio(self, audio_data):\n        """\n        Apply preprocessing to improve speech recognition in noisy robot environments\n        """\n        # Step 1: Noise reduction using spectral subtraction\n        enhanced_audio = self._spectral_subtraction(audio_data)\n        \n        # Step 2: Voice activity detection\n        voice_active = self._detect_voice_activity(enhanced_audio)\n        \n        if not voice_active:\n            return None  # No voice detected\n        \n        # Step 3: Audio normalization\n        normalized_audio = self._normalize_audio(enhanced_audio)\n        \n        return normalized_audio\n    \n    def _spectral_subtraction(self, audio_data):\n        """\n        Apply spectral subtraction for noise reduction\n        """\n        # Compute STFT\n        stft = librosa.stft(audio_data, n_fft=self.fft_size, hop_length=self.hop_length)\n        magnitude = np.abs(stft)\n        phase = np.angle(stft)\n        \n        # Estimate noise spectrum from initial frames (assumed to be noise)\n        if len(self.noise_buffer) < self.noise_buffer_size:\n            # If noise estimation is not ready, return original\n            if len(audio_data) > 2048:  # Only add if we have enough samples\n                # Estimate current noise from low-energy frames\n                noise_estimate = np.mean(magnitude[:, :10], axis=1) if magnitude.shape[1] > 10 else np.mean(magnitude, axis=1)\n                self.noise_buffer.append(noise_estimate)\n        \n        # If we have noise estimate, apply spectral subtraction\n        if len(self.noise_buffer) > 0:\n            avg_noise = np.mean(list(self.noise_buffer), axis=0)\n            \n            # Apply spectral subtraction\n            enhanced_magnitude = np.maximum(magnitude - avg_noise[:, np.newaxis], 0.3 * magnitude)\n            \n            # Reconstruct signal\n            enhanced_stft = enhanced_magnitude * np.exp(1j * phase)\n            enhanced_audio = librosa.istft(enhanced_stft, hop_length=self.hop_length, length=len(audio_data))\n            \n            return enhanced_audio\n        \n        return audio_data  # Return original if no noise model available\n    \n    def _detect_voice_activity(self, audio_data):\n        """\n        Detect if voice activity is present using WebRTC VAD\n        """\n        # Convert to the format expected by WebRTC VAD\n        audio_int16 = (audio_data * 32767).astype(np.int16)\n        audio_bytes = audio_int16.tobytes()\n        \n        # Split into frames\n        frames = self._frame_generator(audio_bytes)\n        \n        voice_count = 0\n        total_frames = 0\n        \n        for frame in frames:\n            if self.vad.is_speech(frame, self.sample_rate):\n                voice_count += 1\n            total_frames += 1\n        \n        # Consider voice active if more than 30% of frames contain voice\n        return voice_count / max(1, total_frames) > 0.3\n    \n    def _frame_generator(self, audio_bytes):\n        """Generate audio frames for VAD processing"""\n        frame_size = self.frame_size * 2  # 2 bytes per sample for int16\n        for i in range(0, len(audio_bytes) - frame_size, frame_size):\n            yield audio_bytes[i:i + frame_size]\n    \n    def _normalize_audio(self, audio_data):\n        """Normalize audio to standard level"""\n        # Normalize to -20 dB RMS\n        rms = np.sqrt(np.mean(audio_data ** 2))\n        target_rms = 10 ** (-20 / 20)  # -20 dB\n        \n        if rms > 0:\n            gain = target_rms / rms\n            return audio_data * gain\n        else:\n            return audio_data\n'})}),"\n",(0,i.jsx)(n.h3,{id:"real-time-processing-considerations",children:"Real-time Processing Considerations"}),"\n",(0,i.jsx)(n.p,{children:"Real-time voice processing for robotics requires careful attention to latency and computational efficiency. The system must process audio in real-time while maintaining high accuracy."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class RealTimeVoiceProcessor:\n    def __init__(self, speech_recognizer, audio_preprocessor):\n        self.recognizer = speech_recognizer\n        self.preprocessor = audio_preprocessor\n        \n        # Processing pipeline\n        self.processing_queue = collections.deque(maxlen=10)\n        self.result_cache = {}\n        \n        # Performance metrics\n        self.processing_times = collections.deque(maxlen=50)\n        self.accuracy_history = collections.deque(maxlen=50)\n        \n    def process_realtime_audio(self, audio_chunk):\n        """\n        Process audio chunk in real-time with performance optimization\n        """\n        start_time = time.time()\n        \n        # Preprocess audio\n        preprocessed = self.preprocessor.preprocess_audio(audio_chunk)\n        if preprocessed is None:\n            return None\n        \n        # Add to processing queue\n        self.processing_queue.append({\n            \'audio\': preprocessed,\n            \'timestamp\': time.time()\n        })\n        \n        # Process when queue has enough data\n        if len(self.processing_queue) >= 3:  # Process in batches for efficiency\n            batch_result = self._process_batch()\n            processing_time = time.time() - start_time\n            self.processing_times.append(processing_time)\n            \n            return batch_result\n        \n        return None\n    \n    def _process_batch(self):\n        """Process multiple audio segments efficiently"""\n        batch_audio = np.concatenate([item[\'audio\'] for item in self.processing_queue])\n        self.processing_queue.clear()\n        \n        # Process the combined audio\n        result = self.recognizer.process_audio_segment(batch_audio)\n        \n        return result\n    \n    def get_performance_metrics(self):\n        """Get real-time performance metrics"""\n        if not self.processing_times:\n            return None\n            \n        avg_processing_time = sum(self.processing_times) / len(self.processing_times)\n        \n        return {\n            \'avg_processing_time\': avg_processing_time,\n            \'processing_rate\': 1.0 / avg_processing_time if avg_processing_time > 0 else float(\'inf\'),\n            \'queue_depth\': len(self.processing_queue),\n            \'sample_rate\': self.recognizer.sample_rate\n        }\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"103-natural-language-understanding-for-robot-commands",children:"10.3 Natural Language Understanding for Robot Commands"}),"\n",(0,i.jsx)(n.h3,{id:"command-structure-and-intent-recognition",children:"Command Structure and Intent Recognition"}),"\n",(0,i.jsx)(n.p,{children:"Natural language understanding for robotics requires specialized processing to map natural language commands to executable robot actions. Unlike general-purpose language understanding, robot command understanding must account for the physical nature of robot capabilities and environmental context."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import re\nimport spacy\nfrom typing import Dict, List, Tuple, Optional\nimport inflect\n\nclass RobotCommandParser:\n    def __init__(self):\n        # Load spaCy English model (you may need to install it: python -m spacy download en_core_web_sm)\n        try:\n            self.nlp = spacy.load(\"en_core_web_sm\")\n        except OSError:\n            print(\"spaCy English model not found. Please install with: python -m spacy download en_core_web_sm\")\n            self.nlp = None\n        \n        self.inflect_engine = inflect.engine()\n        \n        # Define robot action vocabulary\n        self.action_verbs = {\n            'move': ['go', 'move', 'navigate', 'walk', 'drive', 'travel'],\n            'grasp': ['pick up', 'take', 'grasp', 'grab', 'lift', 'collect'],\n            'place': ['put', 'place', 'set', 'drop', 'release', 'position'],\n            'bring': ['bring', 'fetch', 'carry', 'transport', 'deliver'],\n            'follow': ['follow', 'accompany', 'accompany', 'go after'],\n            'stop': ['stop', 'halt', 'wait', 'pause'],\n            'look_at': ['look at', 'see', 'observe', 'examine', 'view'],\n            'point_to': ['point to', 'point at', 'indicate', 'show'],\n        }\n        \n        # Object types and categories\n        self.object_categories = {\n            'container': ['cup', 'bottle', 'box', 'bowl', 'glass', 'mug'],\n            'food': ['apple', 'banana', 'snack', 'cookie', 'fruit', 'sandwich'],\n            'tool': ['pen', 'book', 'phone', 'keys', 'tablet', 'remote'],\n            'furniture': ['table', 'chair', 'couch', 'desk', 'shelf', 'bed'],\n        }\n        \n        # Spatial relationships and directions\n        self.spatial_relations = [\n            'near', 'next to', 'beside', 'on', 'in', 'at', 'by', 'to', 'from',\n            'left', 'right', 'front', 'back', 'above', 'below', 'in front of', \n            'behind', 'across from', 'around'\n        ]\n        \n        # Location references\n        self.location_keywords = [\n            'kitchen', 'living room', 'bedroom', 'office', 'bathroom', \n            'dining room', 'hallway', 'door', 'window', 'cabinet'\n        ]\n\n    def parse_command(self, text: str) -> Optional[Dict]:\n        \"\"\"\n        Parse natural language command to extract action, objects, and spatial relationships\n        \"\"\"\n        if not self.nlp:\n            return None\n            \n        doc = self.nlp(text.lower())\n        \n        # Extract primary action\n        action = self._extract_action(doc)\n        if not action:\n            return None\n        \n        # Extract objects to manipulate\n        objects = self._extract_objects(doc)\n        \n        # Extract spatial information\n        spatial_info = self._extract_spatial_info(doc)\n        \n        # Extract destination/locations\n        locations = self._extract_locations(doc)\n        \n        # Extract modifiers (quantifiers, colors, sizes)\n        modifiers = self._extract_modifiers(doc)\n        \n        command_structure = {\n            'action': action,\n            'objects': objects,\n            'spatial_info': spatial_info,\n            'locations': locations,\n            'modifiers': modifiers,\n            'original_text': text,\n            'confidence': self._calculate_parse_confidence(doc)\n        }\n        \n        return command_structure\n    \n    def _extract_action(self, doc) -> Optional[str]:\n        \"\"\"Extract the primary action verb from the command\"\"\"\n        for token in doc:\n            # Check if token matches any action verb pattern\n            current_span = 1\n            while current_span <= 3 and len(doc) > token.i + current_span - 1:\n                span_text = ' '.join([t.text for t in doc[token.i:token.i + current_span]])\n                \n                for action_type, verbs in self.action_verbs.items():\n                    if span_text in verbs or token.lemma_ in verbs:\n                        return action_type\n                \n                current_span += 1\n            \n            # Also check single token if span didn't match\n            if token.pos_ == 'VERB' or token.lemma_ in [v for verbs in self.action_verbs.values() for v in verbs]:\n                for action_type, verbs in self.action_verbs.items():\n                    if token.lemma_ in verbs:\n                        return action_type\n        \n        return None\n    \n    def _extract_objects(self, doc) -> List[Dict]:\n        \"\"\"Extract objects mentioned in the command\"\"\"\n        objects = []\n        \n        # Look for noun phrases and entities\n        for chunk in doc.noun_chunks:\n            # Check if this chunk is describing an object\n            if self._is_physical_object(chunk.root):\n                obj_info = {\n                    'text': chunk.text,\n                    'lemma': chunk.root.lemma_,\n                    'category': self._categorize_object(chunk.root.lemma_),\n                    'modifiers': []\n                }\n                \n                # Look for adjectives that modify the object\n                for token in chunk:\n                    if token.pos_ == 'ADJ':\n                        obj_info['modifiers'].append(token.text)\n                \n                objects.append(obj_info)\n        \n        return objects\n    \n    def _extract_spatial_info(self, doc) -> List[Dict]:\n        \"\"\"Extract spatial relationships and positioning information\"\"\"\n        spatial_info = []\n        \n        # Look for prepositional phrases indicating spatial relationships\n        for token in doc:\n            if token.pos_ == 'ADP' and token.text in self.spatial_relations:\n                # Extract what the preposition relates to and what it refers to\n                related_to = token.head.text if token.head else None\n                reference_point = None\n                \n                # Look for the object of the preposition\n                for child in token.children:\n                    if child.pos_ in ['NOUN', 'PROPN', 'PRON']:\n                        reference_point = child.text\n                        break\n                \n                spatial_info.append({\n                    'relation': token.text,\n                    'related_to': related_to,\n                    'reference_point': reference_point\n                })\n        \n        return spatial_info\n    \n    def _extract_locations(self, doc) -> List[str]:\n        \"\"\"Extract location information from the command\"\"\"\n        locations = []\n        \n        for token in doc:\n            if any(location in token.text for location in self.location_keywords):\n                locations.append(token.text)\n        \n        # Also check noun chunks that might contain location info\n        for chunk in doc.noun_chunks:\n            if any(location in chunk.text.lower() for location in self.location_keywords):\n                locations.append(chunk.text)\n        \n        return list(set(locations))  # Remove duplicates\n    \n    def _extract_modifiers(self, doc) -> List[Dict]:\n        \"\"\"Extract quantity and quality modifiers\"\"\"\n        modifiers = []\n        \n        for token in doc:\n            if token.pos_ in ['NUM', 'ADJ', 'DET']:\n                modifier_info = {\n                    'type': token.pos_,\n                    'text': token.text,\n                    'value': self._parse_number(token.text) if token.pos_ == 'NUM' else token.text\n                }\n                modifiers.append(modifier_info)\n        \n        return modifiers\n    \n    def _is_physical_object(self, token) -> bool:\n        \"\"\"Determine if a token likely refers to a physical object\"\"\"\n        # Check if it's in known object categories\n        if token.lemma_ in [obj for category in self.object_categories.values() for obj in category]:\n            return True\n        \n        # Check if it's a common physical object (nouns)\n        if token.pos_ in ['NOUN', 'PROPN'] and token.pos_ != 'SCONJ':\n            return True\n        \n        return False\n    \n    def _categorize_object(self, obj_text) -> str:\n        \"\"\"Categorize an object based on its name\"\"\"\n        for category, objects in self.object_categories.items():\n            if obj_text in objects:\n                return category\n        \n        return 'other'\n    \n    def _parse_number(self, num_text) -> int:\n        \"\"\"Parse number text to integer value\"\"\"\n        try:\n            return int(num_text)\n        except ValueError:\n            # Handle written numbers\n            try:\n                return self.inflect_engine.number_to_words(num_text)\n            except:\n                return 0  # Default to 0 if can't parse\n    \n    def _calculate_parse_confidence(self, doc) -> float:\n        \"\"\"Calculate confidence in the parse based on various factors\"\"\"\n        confidence = 0.5  # Base confidence\n        \n        # Increase confidence if we found a clear action\n        if self._extract_action(doc):\n            confidence += 0.3\n        \n        # Increase confidence based on number of objects found\n        objects = self._extract_objects(doc)\n        confidence += min(0.2, len(objects) * 0.05)\n        \n        # Consider sentence length (too short might be ambiguous, too long might be complex)\n        if 3 <= len(doc) <= 15:\n            confidence += 0.1\n        \n        return min(1.0, confidence)\n\n# Example usage\nparser = RobotCommandParser()\n\n# Test command parsing\ntest_commands = [\n    \"Pick up the red cup from the table\",\n    \"Go to the kitchen and bring me a bottle of water\",\n    \"Point to the blue book on the shelf\",\n    \"Place the box next to the chair\"\n]\n\nfor cmd in test_commands:\n    result = parser.parse_command(cmd)\n    if result:\n        print(f\"Command: {cmd}\")\n        print(f\"Parsed: {result}\")\n        print(\"-\" * 50)\n"})}),"\n",(0,i.jsx)(n.h3,{id:"semantic-role-labeling-for-action-understanding",children:"Semantic Role Labeling for Action Understanding"}),"\n",(0,i.jsx)(n.p,{children:"For more sophisticated command understanding, we can implement semantic role labeling to identify the roles of different entities in robot commands."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class SemanticRoleLabeler:\n    def __init__(self):\n        # Define semantic roles relevant to robotics\n        self.semantic_roles = {\n            'Agent': 'The entity performing the action (usually the robot)',\n            'Patient': 'The entity being acted upon (object to manipulate)',\n            'Theme': 'The entity being moved or affected',\n            'Source': 'The starting location of action',\n            'Goal': 'The destination or target of action',\n            'Instrument': 'The tool used to perform action',\n            'Location': 'The place where action occurs',\n            'Direction': 'The direction of movement',\n            'Time': 'When the action should occur'\n        }\n    \n    def label_command_roles(self, command_structure: Dict) -> Dict:\n        \"\"\"\n        Assign semantic roles to elements in a parsed command\n        \"\"\"\n        roles = {\n            'Agent': 'robot',  # Robot is usually the agent\n            'Patient': [],\n            'Theme': [],\n            'Source': [],\n            'Goal': [],\n            'Instrument': [],\n            'Location': [],\n            'Direction': [],\n            'Time': []\n        }\n        \n        # Assign roles based on command structure\n        if command_structure['objects']:\n            roles['Patient'] = command_structure['objects']\n            roles['Theme'] = command_structure['objects']\n        \n        if command_structure['locations']:\n            # Determine if locations are source or goal based on action type\n            if command_structure['action'] in ['move', 'go', 'navigate']:\n                roles['Goal'] = command_structure['locations']\n            elif command_structure['action'] in ['grasp', 'take']:\n                roles['Source'] = command_structure['locations']\n        \n        # Extract directional information from spatial info\n        for spatial in command_structure['spatial_info']:\n            if spatial['relation'] in ['left', 'right', 'front', 'back', 'above', 'below']:\n                roles['Direction'].append(spatial['relation'])\n        \n        # Combine with original structure\n        command_structure['semantic_roles'] = roles\n        return command_structure\n\n# Integration example\nrole_labeler = SemanticRoleLabeler()\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"104-voice-command-mapping-to-robot-actions",children:"10.4 Voice Command Mapping to Robot Actions"}),"\n",(0,i.jsx)(n.h3,{id:"action-mapping-architecture",children:"Action Mapping Architecture"}),"\n",(0,i.jsx)(n.p,{children:"The process of mapping voice commands to robot actions requires a sophisticated architecture that can handle various command types and translate them into executable robot behaviors. This mapping must consider the robot's current state, capabilities, and environmental constraints."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import asyncio\nfrom enum import Enum\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Any, Callable\nimport numpy as np\n\nclass RobotActionType(Enum):\n    NAVIGATION = \"navigation\"\n    MANIPULATION = \"manipulation\"\n    PERCEPTION = \"perception\"\n    COMMUNICATION = \"communication\"\n    WAIT = \"wait\"\n\n@dataclass\nclass RobotCommand:\n    action_type: RobotActionType\n    parameters: Dict[str, Any]\n    priority: int = 5  # 1-10, higher is more urgent\n    timeout: float = 30.0  # seconds\n    feedback_required: bool = True\n\nclass ActionMapper:\n    def __init__(self, robot_interface):\n        self.robot_interface = robot_interface\n        \n        # Action mapping rules\n        self.action_mapping = {\n            'move': self._map_navigation,\n            'go': self._map_navigation, \n            'navigate': self._map_navigation,\n            'grasp': self._map_manipulation,\n            'pick up': self._map_manipulation,\n            'take': self._map_manipulation,\n            'place': self._map_placement,\n            'put': self._map_placement,\n            'look_at': self._map_perception,\n            'examine': self._map_perception,\n            'point_to': self._map_communication,\n            'stop': self._map_wait,\n            'wait': self._map_wait\n        }\n        \n        # Capability constraints\n        self.capabilities = robot_interface.get_capabilities()\n    \n    def map_command(self, parsed_command: Dict) -> List[RobotCommand]:\n        \"\"\"\n        Map a parsed command to executable robot commands\n        \"\"\"\n        action_type = parsed_command['action']\n        objects = parsed_command['objects']\n        spatial_info = parsed_command['spatial_info']\n        locations = parsed_command['locations']\n        modifiers = parsed_command['modifiers']\n        \n        if action_type in self.action_mapping:\n            # Call the appropriate mapping function\n            robot_commands = self.action_mapping[action_type](\n                objects, spatial_info, locations, modifiers, parsed_command\n            )\n            return robot_commands\n        else:\n            raise ValueError(f\"Unknown action type: {action_type}\")\n    \n    def _map_navigation(self, objects, spatial_info, locations, modifiers, original_cmd) -> List[RobotCommand]:\n        \"\"\"Map navigation-related commands to robot actions\"\"\"\n        commands = []\n        \n        # Determine destination\n        destination = None\n        if locations:\n            destination = self._resolve_location(locations[0])\n        elif spatial_info:\n            # Use spatial information to determine navigation target\n            destination = self._interpret_spatial_navigation(spatial_info)\n        \n        if destination:\n            commands.append(RobotCommand(\n                action_type=RobotActionType.NAVIGATION,\n                parameters={'destination': destination, 'speed': 0.5},\n                priority=7,\n                timeout=60.0\n            ))\n        \n        return commands\n    \n    def _map_manipulation(self, objects, spatial_info, locations, modifiers, original_cmd) -> List[RobotCommand]:\n        \"\"\"Map manipulation-related commands to robot actions\"\"\"\n        commands = []\n        \n        if not objects:\n            print(\"No object specified for manipulation command\")\n            return commands\n        \n        # Find the target object\n        target_object = objects[0]  # Use first object as primary target\n        \n        # Get object location (may need perception to locate it)\n        commands.append(RobotCommand(\n            action_type=RobotActionType.PERCEPTION,\n            parameters={'search_for': target_object['text'], 'search_type': 'object'},\n            priority=9,\n            timeout=10.0\n        ))\n        \n        # Grasp action\n        commands.append(RobotCommand(\n            action_type=RobotActionType.MANIPULATION,\n            parameters={'action': 'grasp', 'object': target_object, 'location': 'auto'},\n            priority=8,\n            timeout=15.0\n        ))\n        \n        return commands\n    \n    def _map_placement(self, objects, spatial_info, locations, modifiers, original_cmd) -> List[RobotCommand]:\n        \"\"\"Map placement-related commands to robot actions\"\"\"\n        commands = []\n        \n        # Usually involves placing a held object\n        if locations:\n            place_location = self._resolve_location(locations[0])\n            commands.append(RobotCommand(\n                action_type=RobotActionType.MANIPULATION,\n                parameters={'action': 'place', 'location': place_location},\n                priority=8,\n                timeout=15.0\n            ))\n        \n        return commands\n    \n    def _map_perception(self, objects, spatial_info, locations, modifiers, original_cmd) -> List[RobotCommand]:\n        \"\"\"Map perception-related commands to robot actions\"\"\"\n        commands = []\n        \n        if objects:\n            target = objects[0]['text']\n            commands.append(RobotCommand(\n                action_type=RobotActionType.PERCEPTION,\n                parameters={'action': 'look_at', 'target': target},\n                priority=6,\n                timeout=10.0\n            ))\n        elif spatial_info:\n            direction = spatial_info[0].get('reference_point', 'forward')\n            commands.append(RobotCommand(\n                action_type=RobotActionType.PERCEPTION,\n                parameters={'action': 'look_direction', 'direction': direction},\n                priority=6,\n                timeout=10.0\n            ))\n        \n        return commands\n    \n    def _map_communication(self, objects, spatial_info, locations, modifiers, original_cmd) -> List[RobotCommand]:\n        \"\"\"Map communication-related commands (pointing, gestures)\"\"\"\n        commands = []\n        \n        if objects:\n            target = objects[0]['text']\n            commands.append(RobotCommand(\n                action_type=RobotActionType.COMMUNICATION,\n                parameters={'action': 'point_to', 'target': target},\n                priority=5,\n                timeout=5.0\n            ))\n        \n        return commands\n    \n    def _map_wait(self, objects, spatial_info, locations, modifiers, original_cmd) -> List[RobotCommand]:\n        \"\"\"Map wait-related commands\"\"\"\n        commands = []\n        \n        # Determine wait duration from modifiers if available\n        duration = 5.0  # default\n        for mod in modifiers:\n            if mod['type'] == 'NUM':\n                duration = float(mod['value'])\n                break\n        \n        commands.append(RobotCommand(\n            action_type=RobotActionType.WAIT,\n            parameters={'duration': duration},\n            priority=3,\n            timeout=duration + 2.0\n        ))\n        \n        return commands\n    \n    def _resolve_location(self, location_text: str) -> Dict:\n        \"\"\"Resolve a location reference to coordinates or map location\"\"\"\n        # In a real implementation, this would query a map or spatial memory\n        # For now, using a simple example resolution\n        location_map = {\n            'kitchen': {'x': 5.0, 'y': 3.0, 'z': 0.0},\n            'living room': {'x': 2.0, 'y': 1.0, 'z': 0.0},\n            'bedroom': {'x': 8.0, 'y': 1.0, 'z': 0.0},\n            'office': {'x': 4.0, 'y': 6.0, 'z': 0.0},\n        }\n        \n        return location_map.get(location_text.lower(), {'x': 0.0, 'y': 0.0, 'z': 0.0})\n    \n    def _interpret_spatial_navigation(self, spatial_info: List[Dict]) -> Dict:\n        \"\"\"Interpret spatial relationships for navigation\"\"\"\n        # This would implement spatial reasoning for navigation\n        # Simplified example\n        if spatial_info:\n            rel_info = spatial_info[0]\n            if rel_info['relation'] in ['left', 'right', 'front', 'back']:\n                # Move in the relative direction from current position\n                current_pos = self.robot_interface.get_current_position()\n                if rel_info['relation'] == 'left':\n                    return {'x': current_pos['x'] - 1.0, 'y': current_pos['y'], 'z': current_pos['z']}\n                elif rel_info['relation'] == 'right':\n                    return {'x': current_pos['x'] + 1.0, 'y': current_pos['y'], 'z': current_pos['z']}\n                # ... other directions\n        \n        return {'x': 0.0, 'y': 0.0, 'z': 0.0}\n\nclass RobotActionExecutor:\n    def __init__(self, robot_interface, action_mapper):\n        self.robot_interface = robot_interface\n        self.action_mapper = action_mapper\n        self.command_queue = asyncio.Queue()\n        self.action_history = []\n    \n    async def execute_command(self, parsed_command: Dict) -> Dict:\n        \"\"\"\n        Execute a fully parsed voice command\n        \"\"\"\n        try:\n            # Map the command to robot actions\n            robot_commands = self.action_mapper.map_command(parsed_command)\n            \n            # Execute each command in sequence\n            results = []\n            for cmd in robot_commands:\n                result = await self._execute_single_command(cmd)\n                results.append(result)\n                \n                # Check for failure - stop execution if needed\n                if not result.get('success', True):\n                    break\n            \n            execution_result = {\n                'success': all(r.get('success', False) for r in results),\n                'command': parsed_command,\n                'actions': results,\n                'timestamp': time.time()\n            }\n            \n            self.action_history.append(execution_result)\n            return execution_result\n            \n        except Exception as e:\n            error_result = {\n                'success': False,\n                'error': str(e),\n                'command': parsed_command,\n                'actions': [],\n                'timestamp': time.time()\n            }\n            self.action_history.append(error_result)\n            return error_result\n    \n    async def _execute_single_command(self, robot_cmd: RobotCommand) -> Dict:\n        \"\"\"Execute a single robot command\"\"\"\n        start_time = time.time()\n        \n        try:\n            if robot_cmd.action_type == RobotActionType.NAVIGATION:\n                result = await self._execute_navigation(robot_cmd)\n            elif robot_cmd.action_type == RobotActionType.MANIPULATION:\n                result = await self._execute_manipulation(robot_cmd)\n            elif robot_cmd.action_type == RobotActionType.PERCEPTION:\n                result = await self._execute_perception(robot_cmd)\n            elif robot_cmd.action_type == RobotActionType.COMMUNICATION:\n                result = await self._execute_communication(robot_cmd)\n            elif robot_cmd.action_type == RobotActionType.WAIT:\n                result = await self._execute_wait(robot_cmd)\n            else:\n                result = {'success': False, 'error': 'Unknown action type'}\n        \n        except asyncio.TimeoutError:\n            result = {'success': False, 'error': 'Command timed out'}\n        except Exception as e:\n            result = {'success': False, 'error': f'Execution error: {str(e)}'}\n        \n        # Add execution time\n        result['execution_time'] = time.time() - start_time\n        result['command'] = robot_cmd\n        \n        return result\n    \n    async def _execute_navigation(self, robot_cmd: RobotCommand) -> Dict:\n        \"\"\"Execute navigation command\"\"\"\n        destination = robot_cmd.parameters['destination']\n        speed = robot_cmd.parameters.get('speed', 0.5)\n        \n        # In real implementation, this would call navigation stack\n        # For simulation:\n        print(f\"Navigating to destination: {destination}\")\n        \n        # Simulate navigation completion\n        await asyncio.sleep(1.0)  # Simulate navigation time\n        \n        return {'success': True, 'destination': destination, 'path_length': 2.0}\n    \n    async def _execute_manipulation(self, robot_cmd: RobotCommand) -> Dict:\n        \"\"\"Execute manipulation command\"\"\"\n        action = robot_cmd.parameters['action']\n        obj = robot_cmd.parameters.get('object', 'unknown object')\n        \n        print(f\"Performing manipulation: {action} {obj}\")\n        \n        # Simulate manipulation completion\n        await asyncio.sleep(2.0)  # Simulate manipulation time\n        \n        return {'success': True, 'action': action, 'object': obj}\n    \n    async def _execute_perception(self, robot_cmd: RobotCommand) -> Dict:\n        \"\"\"Execute perception command\"\"\"\n        action = robot_cmd.parameters.get('action', 'look')\n        target = robot_cmd.parameters.get('target', 'environment')\n        \n        print(f\"Performing perception: {action} at {target}\")\n        \n        # Simulate perception completion\n        await asyncio.sleep(0.5)  # Simulate perception time\n        \n        # Simulate perception results\n        perception_data = {\n            'objects_detected': 3,\n            'target_found': True,\n            'confidence': 0.85\n        }\n        \n        return {'success': True, 'action': action, 'target': target, 'data': perception_data}\n    \n    async def _execute_communication(self, robot_cmd: RobotCommand) -> Dict:\n        \"\"\"Execute communication command\"\"\"\n        action = robot_cmd.parameters['action']\n        target = robot_cmd.parameters.get('target', 'unknown')\n        \n        print(f\"Performing communication: {action} at {target}\")\n        \n        # Simulate communication completion\n        await asyncio.sleep(0.3)\n        \n        return {'success': True, 'action': action, 'target': target}\n    \n    async def _execute_wait(self, robot_cmd: RobotCommand) -> Dict:\n        \"\"\"Execute wait command\"\"\"\n        duration = robot_cmd.parameters['duration']\n        \n        print(f\"Waiting for {duration} seconds\")\n        \n        await asyncio.sleep(duration)\n        \n        return {'success': True, 'duration': duration}\n\n# Example integration\nclass VoiceCommandSystem:\n    def __init__(self, robot_interface):\n        self.speech_recognizer = SpeechRecognizer()\n        self.audio_preprocessor = AudioPreprocessor()\n        self.command_parser = RobotCommandParser()\n        self.role_labeler = SemanticRoleLabeler()\n        self.action_mapper = ActionMapper(robot_interface)\n        self.action_executor = RobotActionExecutor(robot_interface, self.action_mapper)\n        \n        # Real-time processing components\n        self.realtime_processor = RealTimeVoiceProcessor(\n            self.speech_recognizer, \n            self.audio_preprocessor\n        )\n    \n    async def process_voice_command(self, audio_chunk=None) -> Dict:\n        \"\"\"Process a voice command from audio to action execution\"\"\"\n        if audio_chunk is not None:\n            # Process real-time audio\n            processed_audio = self.realtime_processor.process_realtime_audio(audio_chunk)\n        else:\n            # Wait for command from microphone\n            print(\"Listening for voice command...\")\n            self.speech_recognizer.start_listening()\n            \n            # In a real implementation, you would have a callback mechanism\n            # For now, simulate with a recognized command\n            recognized_text = \"Go to the kitchen and pick up the red cup\"\n            parsed = self.command_parser.parse_command(recognized_text)\n            \n            if parsed:\n                # Apply semantic role labeling\n                labeled_command = self.role_labeler.label_command_roles(parsed)\n                \n                # Execute the command\n                result = await self.action_executor.execute_command(labeled_command)\n                \n                return result\n            else:\n                return {'success': False, 'error': 'Could not parse command'}\n"})}),"\n",(0,i.jsx)(n.h3,{id:"context-aware-command-resolution",children:"Context-Aware Command Resolution"}),"\n",(0,i.jsx)(n.p,{children:"Physical AI systems need to resolve ambiguous commands using environmental context and current robot state."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class ContextResolver:\n    def __init__(self):\n        self.robot_state = {}\n        self.environment_map = {}\n        self.object_memory = {}\n        self.spatial_memory = {}\n    \n    def resolve_ambiguous_command(self, parsed_command: Dict, context: Dict) -> Dict:\n        \"\"\"\n        Resolve ambiguous elements in a command using context\n        \"\"\"\n        resolved_command = parsed_command.copy()\n        \n        # Resolve ambiguous objects using environment context\n        for i, obj in enumerate(parsed_command['objects']):\n            resolved_obj = self._resolve_object_reference(obj, context)\n            resolved_command['objects'][i] = resolved_obj\n        \n        # Resolve ambiguous locations using spatial context\n        for i, location in enumerate(parsed_command['locations']):\n            resolved_location = self._resolve_location_reference(location, context)\n            resolved_command['locations'][i] = resolved_location\n        \n        # Resolve spatial relationships using current robot state\n        for i, spatial in enumerate(parsed_command['spatial_info']):\n            resolved_spatial = self._resolve_spatial_reference(spatial, context)\n            resolved_command['spatial_info'][i] = resolved_spatial\n        \n        return resolved_command\n    \n    def _resolve_object_reference(self, obj: Dict, context: Dict) -> Dict:\n        \"\"\"\n        Resolve object reference using environment context\n        \"\"\"\n        # Get all objects currently perceived\n        perceived_objects = context.get('perceived_objects', [])\n        \n        # Look for objects matching the description\n        candidates = []\n        for p_obj in perceived_objects:\n            if self._match_object_description(obj, p_obj):\n                candidates.append(p_obj)\n        \n        if candidates:\n            # Use the closest or most recently seen object\n            closest = min(candidates, key=lambda x: x.get('distance', float('inf')))\n            obj['resolved_object'] = closest\n            obj['confidence'] = 0.9  # High confidence in resolution\n        else:\n            obj['confidence'] = 0.3  # Low confidence, object not found\n            obj['resolved_object'] = None\n        \n        return obj\n    \n    def _resolve_location_reference(self, location: str, context: Dict) -> str:\n        \"\"\"\n        Resolve location reference using spatial memory\n        \"\"\"\n        # Check if location is in known spatial memory\n        known_locations = context.get('known_locations', {})\n        \n        if location in known_locations:\n            return known_locations[location]\n        \n        # Fallback to map-based resolution\n        # This would involve spatial reasoning to match natural language\n        # to specific coordinates or map locations\n        return location  # Return original if cannot resolve\n    \n    def _resolve_spatial_reference(self, spatial: Dict, context: Dict) -> Dict:\n        \"\"\"\n        Resolve spatial reference using robot state and environment\n        \"\"\"\n        # Get robot's current pose\n        robot_pose = context.get('robot_pose', {'x': 0, 'y': 0, 'theta': 0})\n        \n        # Calculate actual coordinates based on relative reference\n        if spatial['relation'] in ['left', 'right', 'front', 'back']:\n            angle_offset = self._get_direction_offset(spatial['relation'])\n            target_angle = robot_pose['theta'] + angle_offset\n            target_x = robot_pose['x'] + np.cos(target_angle) * 1.0  # 1m offset\n            target_y = robot_pose['y'] + np.sin(target_angle) * 1.0\n            \n            spatial['resolved_coordinates'] = {'x': target_x, 'y': target_y}\n        \n        return spatial\n    \n    def _match_object_description(self, target_desc: Dict, candidate_obj: Dict) -> bool:\n        \"\"\"\n        Match an object description to a candidate object\n        \"\"\"\n        # Check if object name matches\n        if target_desc['lemma'] == candidate_obj.get('name', '').lower():\n            return True\n        \n        # Check category match\n        if target_desc.get('category') == candidate_obj.get('category'):\n            # Check color or other modifiers\n            target_color = self._extract_color(target_desc.get('modifiers', []))\n            candidate_color = candidate_obj.get('color')\n            \n            if target_color and candidate_color:\n                return target_color.lower() == candidate_color.lower()\n            else:\n                return True  # Category match without color conflict\n        \n        return False\n    \n    def _extract_color(self, modifiers: List[str]) -> str:\n        \"\"\"Extract color from object modifiers\"\"\"\n        color_words = ['red', 'blue', 'green', 'yellow', 'black', 'white', 'orange', 'purple', 'pink', 'gray']\n        for mod in modifiers:\n            if mod.lower() in color_words:\n                return mod.lower()\n        return None\n    \n    def _get_direction_offset(self, direction: str) -> float:\n        \"\"\"Get angle offset for relative directions\"\"\"\n        offsets = {\n            'front': 0.0,\n            'ahead': 0.0,\n            'forward': 0.0,\n            'front_left': np.pi/4,\n            'front_right': -np.pi/4,\n            'left': np.pi/2,\n            'right': -np.pi/2,\n            'back_left': 3*np.pi/4,\n            'back_right': -3*np.pi/4,\n            'back': np.pi\n        }\n        return offsets.get(direction, 0.0)\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"105-robustness-and-error-handling",children:"10.5 Robustness and Error Handling"}),"\n",(0,i.jsx)(n.h3,{id:"handling-recognition-errors",children:"Handling Recognition Errors"}),"\n",(0,i.jsx)(n.p,{children:"Voice-to-action systems must gracefully handle recognition errors and provide appropriate feedback to users. Error handling strategies include confidence-based filtering, error correction, and user clarification requests."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class RobustVoiceProcessor:\n    def __init__(self):\n        self.confidence_threshold = 0.7\n        self.recent_errors = []\n        self.user_preference_model = {}\n        \n    def handle_recognition_result(self, recognition_result: Dict) -> Dict:\n        \"\"\"\n        Handle speech recognition result with robustness checks\n        \"\"\"\n        if not recognition_result:\n            return self._handle_empty_recognition()\n        \n        confidence = recognition_result.get('confidence', 0.0)\n        text = recognition_result.get('text', '')\n        \n        if confidence < self.confidence_threshold:\n            return self._handle_low_confidence(text, confidence)\n        \n        # Check for common recognition errors\n        corrected_text = self._correct_common_errors(text)\n        \n        return {\n            'success': True,\n            'text': corrected_text,\n            'confidence': confidence,\n            'original_text': text\n        }\n    \n    def _handle_empty_recognition(self) -> Dict:\n        \"\"\"Handle case where no speech was recognized\"\"\"\n        return {\n            'success': False,\n            'error': 'no_speech_detected',\n            'suggested_action': 'try_again',\n            'confidence': 0.0\n        }\n    \n    def _handle_low_confidence(self, text: str, confidence: float) -> Dict:\n        \"\"\"Handle low-confidence recognition results\"\"\"\n        # Check if text makes semantic sense\n        semantic_validity = self._check_semantic_validity(text)\n        \n        if semantic_validity > 0.3:  # Even low confidence, but semantically valid\n            return {\n                'success': True,\n                'text': text,\n                'confidence': confidence,\n                'warning': 'low_confidence',\n                'semantic_validity': semantic_validity\n            }\n        else:\n            return {\n                'success': False,\n                'error': 'low_confidence_and_semantic_invalid',\n                'original_text': text,\n                'confidence': confidence,\n                'suggested_action': 'clarify_or_repeat'\n            }\n    \n    def _correct_common_errors(self, text: str) -> str:\n        \"\"\"\n        Correct common speech recognition errors\n        \"\"\"\n        # Common error patterns and corrections\n        corrections = {\n            # Numbers that sound similar\n            r'\\bfor\\b': 'four',\n            r'\\btoo\\b': 'two',\n            r'\\bwon\\b': 'one',  # \"won\" vs \"one\"\n            \n            # Words that sound similar\n            r'\\bhallway\\b': 'hally',  # If \"hallway\" is misrecognized as \"hally\" in some accents\n            r'\\bcomputer\\b': 'compter',  # Common misrecognition\n        }\n        \n        corrected_text = text\n        for pattern, correction in corrections.items():\n            corrected_text = re.sub(pattern, correction, corrected_text, flags=re.IGNORECASE)\n        \n        return corrected_text\n    \n    def _check_semantic_validity(self, text: str) -> float:\n        \"\"\"\n        Check if recognized text has semantic validity for robot commands\n        \"\"\"\n        # Check if text contains known robot commands\n        robot_command_keywords = [\n            'move', 'go', 'navigate', 'pick', 'take', 'grasp', 'place', 'put',\n            'look', 'see', 'examine', 'point', 'stop', 'wait', 'bring', 'fetch'\n        ]\n        \n        # Count command-related words\n        words = text.lower().split()\n        command_words = [word for word in words if any(cmd in word for cmd in robot_command_keywords)]\n        \n        if len(command_words) >= 1:\n            return min(1.0, len(command_words) / len(words) * 2)  # Boost for command density\n        \n        # Check for object references\n        object_indicators = ['the', 'a', 'an', 'my', 'your', 'this', 'that', 'these', 'those']\n        object_words = [word for word in words if word in object_indicators]\n        \n        return min(0.5, len(object_words) / len(words)) if object_words else 0.0\n    \n    def request_clarification(self, command: Dict, available_options: List[str] = None) -> Dict:\n        \"\"\"\n        Request user clarification for ambiguous commands\n        \"\"\"\n        if available_options:\n            clarification_text = f\"I heard '{command.get('original_text', '')}' but I'm not sure what you mean. Did you mean: {', '.join(available_options)}?\"\n        else:\n            clarification_text = f\"I heard '{command.get('original_text', '')}' but I'm not sure how to help with that. Could you rephrase?\"\n        \n        return {\n            'action': 'request_clarification',\n            'message': clarification_text,\n            'original_command': command,\n            'options': available_options\n        }\n    \n    def learn_from_errors(self, original_recognition: Dict, user_correction: str):\n        \"\"\"\n        Learn from user corrections to improve future recognition\n        \"\"\"\n        # This would implement learning from user corrections\n        # In a real system, this would update acoustic or language models\n        pass\n"})}),"\n",(0,i.jsx)(n.h3,{id:"multi-modal-error-recovery",children:"Multi-Modal Error Recovery"}),"\n",(0,i.jsx)(n.p,{children:"Physical AI systems can leverage multiple modalities to recover from voice recognition errors by combining visual, spatial, and contextual information."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class MultiModalErrorRecovery:\n    def __init__(self):\n        self.perception_system = None\n        self.context_resolver = ContextResolver()\n    \n    def handle_command_error(self, original_command: Dict, error_info: Dict, context: Dict) -> Dict:\n        \"\"\"\n        Handle command execution errors using multi-modal recovery\n        \"\"\"\n        error_type = error_info.get('error_type', 'unknown')\n        \n        if error_type == 'object_not_found':\n            return self._recover_object_not_found(original_command, context)\n        elif error_type == 'location_ambiguous':\n            return self._recover_location_ambiguous(original_command, context)\n        elif error_type == 'action_unsupported':\n            return self._recover_action_unsupported(original_command, context)\n        else:\n            return self._general_recovery(original_command, error_info, context)\n    \n    def _recover_object_not_found(self, command: Dict, context: Dict) -> Dict:\n        \"\"\"\n        Recover when target object is not found using perception\n        \"\"\"\n        # Request active perception to locate possible objects\n        requested_object = command.get('objects', [{}])[0] if command.get('objects') else {}\n        \n        if requested_object:\n            search_params = {\n                'object_type': requested_object.get('lemma', ''),\n                'search_area': context.get('current_location', 'around_robot'),\n                'attributes': requested_object.get('modifiers', [])\n            }\n            \n            # Request perception system to search for objects\n            perception_request = {\n                'action': 'search_for_object',\n                'parameters': search_params,\n                'timeout': 10.0\n            }\n            \n            return {\n                'recovery_action': 'perception_search',\n                'request': perception_request,\n                'original_command': command\n            }\n    \n    def _recover_location_ambiguous(self, command: Dict, context: Dict) -> Dict:\n        \"\"\"\n        Recover when location reference is ambiguous using spatial reasoning\n        \"\"\"\n        # Use spatial context to disambiguate location\n        locations = command.get('locations', [])\n        if not locations:\n            return {'recovery_action': 'ask_for_clarification', 'message': 'Could you specify where exactly?'}\n        \n        # Get all possible locations that match the description\n        possible_locations = self._get_possible_locations(locations[0], context)\n        \n        if len(possible_locations) == 1:\n            # Disambiguated successfully\n            command['locations'] = [possible_locations[0]]\n            return {\n                'recovery_action': 'retry_command',\n                'resolved_command': command\n            }\n        elif len(possible_locations) > 1:\n            # Multiple possibilities, ask user to choose\n            return {\n                'recovery_action': 'request_choice',\n                'options': possible_locations,\n                'message': f\"I found multiple locations that match '{locations[0]}'. Which one did you mean?\"\n            }\n        else:\n            # No matches found\n            return {\n                'recovery_action': 'ask_for_alternative',\n                'message': f\"I don't know a location called '{locations[0]}'. Could you describe it differently?\"\n            }\n    \n    def _recover_action_unsupported(self, command: Dict, context: Dict) -> Dict:\n        \"\"\"\n        Recover when requested action is not supported\n        \"\"\"\n        requested_action = command.get('action')\n        \n        # Provide alternatives based on robot capabilities\n        alternatives = self._get_action_alternatives(requested_action, context)\n        \n        if alternatives:\n            return {\n                'recovery_action': 'suggest_alternatives',\n                'alternatives': alternatives,\n                'message': f\"I can't {requested_action}, but I can {', '.join(alternatives)}. Would you like me to do one of those instead?\"\n            }\n        else:\n            return {\n                'recovery_action': 'inform_limitation',\n                'message': f\"Sorry, I'm not able to {requested_action}. Is there something else I can help with?\"\n            }\n    \n    def _general_recovery(self, command: Dict, error_info: Dict, context: Dict) -> Dict:\n        \"\"\"\n        General error recovery approach\n        \"\"\"\n        # Try to understand intent and suggest alternatives\n        suggested_repair = self._suggest_command_repair(command, error_info, context)\n        \n        if suggested_repair:\n            return {\n                'recovery_action': 'suggest_repair',\n                'suggested_command': suggested_repair,\n                'message': f\"Did you mean: '{suggested_repair}'?\"\n            }\n        else:\n            return {\n                'recovery_action': 'request_rephrase',\n                'message': \"I'm not sure how to help with that. Could you rephrase the command?\"\n            }\n    \n    def _get_possible_locations(self, location_desc: str, context: Dict) -> List[str]:\n        \"\"\"Get possible locations matching a description\"\"\"\n        # This would integrate with spatial memory system\n        # For demo purposes, returning some options\n        if 'room' in location_desc.lower():\n            return ['kitchen', 'living room', 'bedroom', 'office']\n        elif 'table' in location_desc.lower():\n            return ['kitchen table', 'dining table', 'desk']\n        else:\n            return []\n    \n    def _get_action_alternatives(self, requested_action: str, context: Dict) -> List[str]:\n        \"\"\"Get alternative actions for unsupported action\"\"\"\n        # This would check robot capabilities and suggest alternatives\n        capability_map = {\n            'fly': ['move', 'navigate'],\n            'jump': ['step_over', 'climb'],\n            'invisibility': ['hide', 'wait'],\n            'teleport': ['navigate', 'go']\n        }\n        \n        alternatives = capability_map.get(requested_action, [])\n        \n        # Filter by actually available capabilities\n        available_caps = context.get('robot_capabilities', [])\n        return [alt for alt in alternatives if alt in available_caps]\n    \n    def _suggest_command_repair(self, command: Dict, error_info: Dict, context: Dict) -> str:\n        \"\"\"Suggest a repair for the command\"\"\"\n        # Use context and error info to suggest a repair\n        # This would implement more sophisticated NLP-based repair\n        original_text = command.get('original_text', '')\n        \n        # Simple rule-based repairs\n        repairs = {\n            r'bring me the (.+) that (.+)': r'get the \\1 that \\2',\n            r'go and (.+)': r'go to \\1',\n            r'pick up (.+) and move to (.+)': r'take \\1 to \\2'\n        }\n        \n        for pattern, replacement in repairs.items():\n            import re\n            if re.search(pattern, original_text, re.IGNORECASE):\n                repaired = re.sub(pattern, replacement, original_text, re.IGNORECASE)\n                return repaired\n        \n        return None\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"106-integration-with-higher-level-planning-systems",children:"10.6 Integration with Higher-Level Planning Systems"}),"\n",(0,i.jsx)(n.h3,{id:"voice-command-integration-architecture",children:"Voice Command Integration Architecture"}),"\n",(0,i.jsx)(n.p,{children:"The integration between voice command systems and higher-level planning systems requires careful coordination to ensure that voice commands are properly understood, planned, and executed in the context of overall robot goals and capabilities."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class VoiceIntegrationManager:\n    def __init__(self, voice_system, planning_system, execution_system):\n        self.voice_system = voice_system\n        self.planning_system = planning_system\n        self.execution_system = execution_system\n        \n        self.command_history = []\n        self.pending_commands = {}\n        self.user_intents = {}\n        \n    def process_voice_command_as_task(self, command_text: str, user_id: str = \"default\") -> Dict:\n        \"\"\"\n        Process voice command as a high-level task in the planning system\n        \"\"\"\n        # Parse the voice command\n        parsed_command = self.voice_system.command_parser.parse_command(command_text)\n        if not parsed_command:\n            return {'success': False, 'error': 'Could not parse command'}\n        \n        # Apply semantic role labeling\n        labeled_command = self.voice_system.role_labeler.label_command_roles(parsed_command)\n        \n        # Resolve any ambiguities using context\n        context = self._get_current_context()\n        resolved_command = self.voice_system.context_resolver.resolve_ambiguous_command(\n            labeled_command, context\n        )\n        \n        # Convert to planning task\n        planning_task = self._convert_to_planning_task(resolved_command, user_id)\n        \n        # Submit to planning system\n        plan_result = self.planning_system.create_plan(planning_task)\n        \n        if plan_result['success']:\n            # Execute the plan\n            execution_result = self.execution_system.execute_plan(plan_result['plan'])\n            \n            # Update command history\n            command_record = {\n                'command_text': command_text,\n                'parsed_command': resolved_command,\n                'planning_task': planning_task,\n                'plan': plan_result['plan'],\n                'execution_result': execution_result,\n                'timestamp': time.time(),\n                'user_id': user_id\n            }\n            self.command_history.append(command_record)\n            \n            return {\n                'success': True,\n                'planning_result': plan_result,\n                'execution_result': execution_result,\n                'task_id': plan_result.get('task_id')\n            }\n        else:\n            return {\n                'success': False,\n                'planning_error': plan_result.get('error', 'Planning failed'),\n                'command_text': command_text\n            }\n    \n    def _convert_to_planning_task(self, resolved_command: Dict, user_id: str) -> Dict:\n        \"\"\"\n        Convert voice command to planning system task format\n        \"\"\"\n        action = resolved_command['action']\n        \n        # Map voice command action to planning task\n        task_mapping = {\n            'move': 'navigation_task',\n            'grasp': 'manipulation_task', \n            'place': 'placement_task',\n            'bring': 'delivery_task',\n            'look_at': 'perception_task'\n        }\n        \n        task_type = task_mapping.get(action, 'general_task')\n        \n        task = {\n            'task_type': task_type,\n            'action': action,\n            'objects': resolved_command.get('objects', []),\n            'locations': resolved_command.get('locations', []),\n            'spatial_info': resolved_command.get('spatial_info', []),\n            'user_id': user_id,\n            'priority': self._determine_priority(resolved_command),\n            'request_time': time.time()\n        }\n        \n        # Add task-specific parameters\n        if action in ['grasp', 'take']:\n            task['target_object'] = resolved_command['objects'][0] if resolved_command['objects'] else None\n        elif action in ['move', 'go', 'navigate']:\n            task['destination'] = resolved_command['locations'][0] if resolved_command['locations'] else None\n        elif action in ['bring', 'fetch']:\n            task['delivery_target'] = user_id  # Deliver to the user\n            task['pickup_object'] = resolved_command['objects'][0] if resolved_command['objects'] else None\n        \n        return task\n    \n    def _determine_priority(self, command: Dict) -> int:\n        \"\"\"Determine priority of voice command based on various factors\"\"\"\n        priority = 5  # Default medium priority\n        \n        # Increase priority for safety-related commands\n        safety_keywords = ['stop', 'emergency', 'danger', 'help']\n        command_lower = command.get('original_text', '').lower()\n        \n        if any(keyword in command_lower for keyword in safety_keywords):\n            priority = 10  # Highest priority\n        \n        # Adjust based on spatial relationships (immediate vicinity)\n        spatial_info = command.get('spatial_info', [])\n        if any(s['relation'] in ['here', 'this', 'near', 'close'] for s in spatial_info):\n            priority += 1\n        \n        return min(10, max(1, priority))  # Clamp to 1-10 range\n    \n    def _get_current_context(self) -> Dict:\n        \"\"\"Get current context for command resolution\"\"\"\n        return {\n            'robot_pose': self.execution_system.get_robot_state()['pose'],\n            'perceived_objects': self.execution_system.get_perceived_objects(),\n            'known_locations': self.execution_system.get_known_locations(),\n            'robot_capabilities': self.execution_system.get_capabilities(),\n            'current_plan': self.planning_system.get_active_plan(),\n            'task_queue': self.planning_system.get_task_queue()\n        }\n    \n    def handle_conversation_context(self, command_text: str, user_id: str = \"default\") -> Dict:\n        \"\"\"\n        Handle voice commands in the context of ongoing conversation\n        \"\"\"\n        # Check if this continues a previous command\n        if user_id in self.user_intents and self.user_intents[user_id]['status'] == 'awaiting_clarification':\n            # Handle as clarification to previous intent\n            previous_intent = self.user_intents[user_id]\n            response = self._handle_continuation(command_text, previous_intent)\n            return response\n        \n        # Otherwise process as new command\n        return self.process_voice_command_as_task(command_text, user_id)\n    \n    def _handle_continuation(self, response_text: str, previous_intent: Dict) -> Dict:\n        \"\"\"Handle response that continues a previous intent\"\"\"\n        if previous_intent['expected_response_type'] == 'yes_no':\n            if self._is_affirmative(response_text):\n                # Continue with original command\n                return self.process_voice_command_as_task(previous_intent['original_command'], previous_intent['user_id'])\n            else:\n                # Cancel or modify command\n                return {'success': True, 'message': 'Command cancelled', 'cancelled': True}\n        elif previous_intent['expected_response_type'] == 'choice':\n            # Parse the choice from response_text\n            choice = self._extract_choice(response_text, previous_intent['options'])\n            if choice:\n                # Execute command with chosen option\n                return self._execute_with_choice(choice, previous_intent)\n            else:\n                return {'success': False, 'error': 'Could not understand your choice'}\n    \n    def _is_affirmative(self, text: str) -> bool:\n        \"\"\"Check if text is an affirmative response\"\"\"\n        affirmative = ['yes', 'yep', 'sure', 'ok', 'okay', 'go ahead', 'please']\n        return any(word in text.lower() for word in affirmative)\n    \n    def _extract_choice(self, response: str, options: List[str]) -> str:\n        \"\"\"Extract choice from user response\"\"\"\n        # Simple approach: check if any option is in the response\n        response_lower = response.lower()\n        for option in options:\n            if option.lower() in response_lower:\n                return option\n        return None\n    \n    def _execute_with_choice(self, choice: str, previous_intent: Dict) -> Dict:\n        \"\"\"Execute command with user's choice\"\"\"\n        # In practice, this would modify the original command based on the choice\n        # For now, return a success message\n        return {\n            'success': True,\n            'message': f'Executing with choice: {choice}',\n            'choice': choice,\n            'original_intent': previous_intent\n        }\n\n# Example usage in a complete system\nclass CompleteVoiceToActionSystem:\n    def __init__(self):\n        # Initialize all components\n        self.robot_interface = MockRobotInterface()  # Would be real robot interface\n        self.voice_system = VoiceCommandSystem(self.robot_interface)\n        self.planning_system = MockPlanningSystem()  # Would be real planning system\n        self.execution_system = MockExecutionSystem()  # Would be real execution system\n        \n        # Integration manager\n        self.integration_manager = VoiceIntegrationManager(\n            self.voice_system,\n            self.planning_system, \n            self.execution_system\n        )\n        \n        # Error recovery\n        self.error_recovery = MultiModalErrorRecovery()\n        \n        print(\"Complete Voice-to-Action system initialized\")\n    \n    async def process_command_with_recovery(self, command_text: str, user_id: str = \"default\") -> Dict:\n        \"\"\"\n        Process command with full error recovery capabilities\n        \"\"\"\n        try:\n            # First attempt\n            result = self.integration_manager.handle_conversation_context(command_text, user_id)\n            \n            if result.get('success'):\n                return result\n            else:\n                # Handle specific errors with recovery\n                error_type = result.get('error', 'general_failure')\n                context = self.integration_manager._get_current_context()\n                \n                recovery_result = self.error_recovery.handle_command_error(\n                    {'original_text': command_text},\n                    {'error_type': error_type},\n                    context\n                )\n                \n                return {\n                    'original_result': result,\n                    'recovery_attempt': recovery_result,\n                    'needs_user_input': recovery_result.get('recovery_action') in ['request_clarification', 'request_choice']\n                }\n        \n        except Exception as e:\n            return {\n                'success': False,\n                'system_error': str(e),\n                'command': command_text\n            }\n\n# Mock classes for demonstration\nclass MockRobotInterface:\n    def get_capabilities(self):\n        return ['navigation', 'manipulation', 'perception']\n    \n    def get_current_position(self):\n        return {'x': 0.0, 'y': 0.0, 'z': 0.0}\n\nclass MockPlanningSystem:\n    def create_plan(self, task):\n        return {'success': True, 'plan': [{'action': 'demo_action', 'params': {}}], 'task_id': 'task_123'}\n    \n    def get_active_plan(self):\n        return None\n    \n    def get_task_queue(self):\n        return []\n\nclass MockExecutionSystem:\n    def execute_plan(self, plan):\n        return {'success': True, 'actions_completed': 1}\n    \n    def get_robot_state(self):\n        return {'pose': {'x': 0.0, 'y': 0.0, 'theta': 0.0}}\n    \n    def get_perceived_objects(self):\n        return [{'name': 'red cup', 'category': 'container', 'color': 'red', 'distance': 1.5}]\n    \n    def get_known_locations(self):\n        return {'kitchen': {'x': 5.0, 'y': 3.0}, 'living room': {'x': 2.0, 'y': 1.0}}\n    \n    def get_capabilities(self):\n        return ['navigation', 'manipulation', 'perception']\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"This chapter has provided a comprehensive exploration of voice-to-action integration for Physical AI and humanoid robotics systems:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Introduction to voice-to-action"}),": Understanding the role of speech interfaces in natural human-robot interaction"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Speech recognition and audio processing"}),": Implementing modern systems with Whisper and audio preprocessing for robot environments"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Natural language understanding"}),": Parsing natural language commands and extracting semantic meaning"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Action mapping"}),": Converting voice commands to executable robot behaviors with proper context resolution"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Error handling and robustness"}),": Implementing multi-modal error recovery and user interaction management"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"System integration"}),": Connecting voice systems with planning and execution frameworks for seamless operation"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"The key insight from this chapter is that successful voice-to-action integration requires more than just speech recognition; it needs sophisticated natural language understanding, context-aware resolution of ambiguities, and tight integration with robot planning and execution systems. The system must handle the inherent ambiguity of natural language while maintaining the precision required for physical robot control."}),"\n",(0,i.jsx)(n.h2,{id:"key-terms",children:"Key Terms"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Voice-to-Action"}),": Systems that convert spoken language commands to robotic actions"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Automatic Speech Recognition (ASR)"}),": Technology that converts speech to text"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Natural Language Understanding (NLU)"}),": Processing text to extract meaning and intent"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Semantic Role Labeling"}),": Identifying the roles of different entities in actions"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Confidence Scoring"}),": Measuring reliability of speech recognition and command parsing"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Multi-Modal Integration"}),": Combining speech with visual and spatial information"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Context Resolution"}),": Using environmental context to disambiguate commands"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Error Recovery"}),": Strategies for handling recognition and execution failures"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Voice Activity Detection (VAD)"}),": Identifying when speech is present in audio"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Acoustic Model"}),": Component that maps audio features to linguistic units"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Language Model"}),": Component that captures linguistic probability of word sequences"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:'Radford, A., et al. (2022). "Robust Speech Recognition via Large-Scale Weak Supervision." arXiv preprint.'}),"\n",(0,i.jsx)(n.li,{children:'Jurafsky, D., & Martin, J. H. (2020). "Speech and Language Processing" (3rd ed.). Pearson.'}),"\n",(0,i.jsx)(n.li,{children:'Chen, G., et al. (2022). "Transformer-Based Acoustic Modeling for Speech Recognition." IEEE Transactions on Audio, Speech, and Language Processing.'}),"\n",(0,i.jsx)(n.li,{children:'Young, S., et al. (2013). "The Dialog State Tracking Challenge." SIGDIAL Conference.'}),"\n",(0,i.jsx)(n.li,{children:'Kollar, T., et al. (2010). "Toward Understanding Natural Language Spatial Commands for Human-Robot Interaction." IEEE International Conference on Robotics and Automation.'}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Chapter 11 Preview"}),": In the next chapter, we will explore LLM Cognitive Planning for Physical AI systems, examining how large language models can serve as high-level cognitive planners that decompose complex tasks into sequences of executable actions for embodied robots."]})]})}function m(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}}}]);